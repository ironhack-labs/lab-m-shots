{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "24b19fff-8f42-4e9f-a73e-00cff106805a",
      "metadata": {
        "id": "24b19fff-8f42-4e9f-a73e-00cff106805a"
      },
      "source": [
        "# M-Shots Learning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtUjoc17aEIt",
        "outputId": "9d14968a-e969-414e-e060-8460f2232112"
      },
      "id": "UtUjoc17aEIt",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.76.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34723a72-1601-4685-a0ba-bff544425d48",
      "metadata": {
        "id": "34723a72-1601-4685-a0ba-bff544425d48"
      },
      "source": [
        "In this notebook, we'll explore small prompt engineering techniques and recommendations that will help us elicit responses from the models that are better suited to our needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fba193cc-d8a0-4ad2-8177-380204426859",
      "metadata": {
        "id": "fba193cc-d8a0-4ad2-8177-380204426859"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "502cfc93-21e0-498f-9650-37bc6ddd514d",
      "metadata": {
        "id": "502cfc93-21e0-498f-9650-37bc6ddd514d"
      },
      "source": [
        "# Formatting the answer with Few Shot Samples.\n",
        "\n",
        "To obtain the model's response in a specific format, we have various options, but one of the most convenient is to use Few-Shot Samples. This involves presenting the model with pairs of user queries and example responses.\n",
        "\n",
        "Large models like GPT-3.5 respond well to the examples provided, adapting their response to the specified format.\n",
        "\n",
        "Depending on the number of examples given, this technique can be referred to as:\n",
        "* Zero-Shot.\n",
        "* One-Shot.\n",
        "* Few-Shots.\n",
        "\n",
        "With One Shot should be enough, and it is recommended to use a maximum of six shots. It's important to remember that this information is passed in each query and occupies space in the input prompt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a8344712-06d7-4c24-83d8-f36d62926e5e",
      "metadata": {
        "id": "a8344712-06d7-4c24-83d8-f36d62926e5e"
      },
      "outputs": [],
      "source": [
        "# Function to call the model.\n",
        "def return_OAIResponse(user_message, context):\n",
        "    client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=OPENAI_API_KEY,\n",
        ")\n",
        "\n",
        "    newcontext = context.copy()\n",
        "    newcontext.append({'role':'user', 'content':\"question: \" + user_message})\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=newcontext,\n",
        "            temperature=1,\n",
        "        )\n",
        "\n",
        "    return (response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f611d73d-9330-466d-b705-543667e1b561",
      "metadata": {
        "id": "f611d73d-9330-466d-b705-543667e1b561"
      },
      "source": [
        "In this zero-shots prompt we obtain a correct response, but without formatting, as the model incorporates the information he wants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
        "outputId": "d4b67f30-c217-49ed-f828-429cab5d469d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sebastian Vettel won the Formula 1 World Championship in 2010. He drove for the Red Bull Racing team that season.\n"
          ]
        }
      ],
      "source": [
        "#zero-shot\n",
        "context_user = [\n",
        "    {'role':'system', 'content':'You are an expert in F1.'}\n",
        "]\n",
        "print(return_OAIResponse(\"Who won the F1 2010?\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8",
      "metadata": {
        "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8"
      },
      "source": [
        "For a model as large and good as GPT 3.5, a single shot is enough to learn the output format we expect.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
        "outputId": "c052e9f7-e5eb-4c83-d31c-d0df7dd4b473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 2011 F1 World Championship was won by Sebastian Vettel, driving for Red Bull Racing.\n"
          ]
        }
      ],
      "source": [
        "#one-shot\n",
        "context_user = [\n",
        "    {'role':'system', 'content':\n",
        "     \"\"\"You are an expert in F1.\n",
        "\n",
        "     Who won the 2000 f1 championship?\n",
        "     Driver: Michael Schumacher.\n",
        "     Team: Ferrari.\"\"\"}\n",
        "]\n",
        "print(return_OAIResponse(\"Who won the F1 2011?\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32c454a8-181b-482b-873a-81d6ffde4674",
      "metadata": {
        "id": "32c454a8-181b-482b-873a-81d6ffde4674"
      },
      "source": [
        "Smaller models, or more complicated formats, may require more than one shot. Here a sample with two shots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
        "outputId": "f985f342-dcdd-4202-b046-40d16a5ae6c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: Fernando Alonso.\n",
            "Team: Renault.\n"
          ]
        }
      ],
      "source": [
        "#Few shots\n",
        "context_user = [\n",
        "    {'role':'system', 'content':\n",
        "     \"\"\"You are an expert in F1.\n",
        "\n",
        "     Who won the 2010 f1 championship?\n",
        "     Driver: Sebastian Vettel.\n",
        "     Team: Red Bull Renault.\n",
        "\n",
        "     Who won the 2009 f1 championship?\n",
        "     Driver: Jenson Button.\n",
        "     Team: BrawnGP.\"\"\"}\n",
        "]\n",
        "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
        "outputId": "86d601c1-a04d-437a-f80e-dc393c219d84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 2019 F1 World Championship was won by Lewis Hamilton from the Mercedes team.\n"
          ]
        }
      ],
      "source": [
        "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86",
      "metadata": {
        "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86"
      },
      "source": [
        "We've been creating the prompt without using OpenAI's roles, and as we've seen, it worked correctly.\n",
        "\n",
        "However, the proper way to do this is by using these roles to construct the prompt, making the model's learning process even more effective.\n",
        "\n",
        "By not feeding it the entire prompt as if they were system commands, we enable the model to learn from a conversation, which is more realistic for it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
        "outputId": "311da462-67ab-4a53-ee2a-6765484b2992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lewis Hamilton won the Formula 1 2019 championship. He secured his sixth F1 title that year.\n"
          ]
        }
      ],
      "source": [
        "#Recomended solution\n",
        "context_user = [\n",
        "    {'role':'system', 'content':'You are and expert in f1.\\n\\n'},\n",
        "    {'role':'user', 'content':'Who won the 2010 f1 championship?'},\n",
        "    {'role':'assistant', 'content':\"\"\"Driver: Sebastian Vettel. \\nTeam: Red Bull. \\nPoints: 256. \"\"\"},\n",
        "    {'role':'user', 'content':'Who won the 2009 f1 championship?'},\n",
        "    {'role':'assistant', 'content':\"\"\"Driver: Jenson Button. \\nTeam: BrawnGP. \\nPoints: 95. \"\"\"},\n",
        "]\n",
        "\n",
        "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac6f6b42-f351-496b-a7e8-1286426457eb",
      "metadata": {
        "id": "ac6f6b42-f351-496b-a7e8-1286426457eb"
      },
      "source": [
        "We could also address it by using a more conventional prompt, describing what we want and how we want the format.\n",
        "\n",
        "However, it's essential to understand that in this case, the model is following instructions, whereas in the case of use shots, it is learning in real-time during inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
        "outputId": "2690a60f-8276-44e5-fa6a-5701f101483f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: Lewis Hamilton\n",
            "Team: Mercedes\n",
            "Points: 413\n"
          ]
        }
      ],
      "source": [
        "context_user = [\n",
        "    {'role':'system', 'content':\"\"\"You are and expert in f1.\n",
        "    You are going to answer the question of the user giving the name of the rider,\n",
        "    the name of the team and the points of the champion, following the format:\n",
        "    Drive:\n",
        "    Team:\n",
        "    Points: \"\"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "KNDL1GzVngyL",
      "metadata": {
        "id": "KNDL1GzVngyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b0181e4-af3d-404c-9f21-70395287f456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: Fernando Alonso.\n",
            "Team: Renault.\n"
          ]
        }
      ],
      "source": [
        "context_user = [\n",
        "    {'role':'system', 'content':\n",
        "     \"\"\"You are classifying .\n",
        "\n",
        "     Who won the 2010 f1 championship?\n",
        "     Driver: Sebastian Vettel.\n",
        "     Team: Red Bull Renault.\n",
        "\n",
        "     Who won the 2009 f1 championship?\n",
        "     Driver: Jenson Button.\n",
        "     Team: BrawnGP.\"\"\"}\n",
        "]\n",
        "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qZPNTLMPnkQ4",
      "metadata": {
        "id": "qZPNTLMPnkQ4"
      },
      "source": [
        "Few Shots for classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ejcstgTxnnX5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejcstgTxnnX5",
        "outputId": "a23a4d2b-d5bd-4cf2-cda2-b44bc0bee314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: Negative.\n"
          ]
        }
      ],
      "source": [
        "context_user = [\n",
        "    {'role':'system', 'content':\n",
        "     \"\"\"You are an expert in reviewing product opinions and classifying them as positive or negative.\n",
        "\n",
        "     It fulfilled its function perfectly, I think the price is fair, I would buy it again.\n",
        "     Sentiment: Positive\n",
        "\n",
        "     It didn't work bad, but I wouldn't buy it again, maybe it's a bit expensive for what it does.\n",
        "     Sentiment: Negative.\n",
        "\n",
        "     I wouldn't know what to say, my son uses it, but he doesn't love it.\n",
        "     Sentiment: Neutral\n",
        "     \"\"\"}\n",
        "]\n",
        "print(return_OAIResponse(\"I'm not going to return it, but I don't plan to buy it again.\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffe1d50b-d262-4e74-8f2d-3559f3fcfb15",
      "metadata": {
        "id": "ffe1d50b-d262-4e74-8f2d-3559f3fcfb15"
      },
      "source": [
        "# Exercise\n",
        " - Complete the prompts similar to what we did in class.\n",
        "     - Try at least 3 versions\n",
        "     - Be creative\n",
        " - Write a one page report summarizing your findings.\n",
        "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
        " - What did you learn?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5522a64f",
      "metadata": {
        "id": "5522a64f"
      },
      "source": [
        "## Version 1 : Zero-Shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9adda59c-ad09-4e9d-88cd-54f42384a5f3",
      "metadata": {
        "id": "9adda59c-ad09-4e9d-88cd-54f42384a5f3"
      },
      "outputs": [],
      "source": [
        "context_zero_shot = [\n",
        "    {'role': 'system', 'content':\n",
        "     \"\"\"\n",
        "     Your task is to classify product reviews into three categories: Positive, Negative, or Neutral.\n",
        "     Read the given review and decide the sentiment.\n",
        "\n",
        "     \"\"\"}\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2bae169",
      "metadata": {
        "id": "d2bae169"
      },
      "source": [
        "## Version 2 : One-Shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4b00bc7a",
      "metadata": {
        "id": "4b00bc7a"
      },
      "outputs": [],
      "source": [
        "context_one_shot = [\n",
        "    {'role': 'system', 'content':\n",
        "     \"\"\"\n",
        "     Your task is to classify product reviews into three categories: Positive, Negative, or Neutral.\n",
        "     Example:\n",
        "     - Review: \"It didn't work bad, but I wouldn't buy it again, maybe it's a bit expensive for what it does.\"\n",
        "       Sentiment: Negative\n",
        "     \"\"\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7bd54e9",
      "metadata": {
        "id": "a7bd54e9"
      },
      "source": [
        "## Version 3 : Few-Shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cec3208b",
      "metadata": {
        "id": "cec3208b"
      },
      "outputs": [],
      "source": [
        "context_few_shot = [\n",
        "    {'role': 'system', 'content':\n",
        "     \"\"\"\n",
        "     Your task is to classify product reviews into three categories: Positive, Negative, or Neutral.\n",
        "    \"\"\"},\n",
        "\n",
        "    {'role': 'user', 'content': \"The quality is excellent, I would definitely buy it again!\"},\n",
        "    {'role': 'assistant', 'content': 'Sentiment: Positive'},\n",
        "\n",
        "    {'role': 'user', 'content': \"It stopped working after just one week! Not worth the money.\"},\n",
        "    {'role': 'assistant', 'content': 'Sentiment: Negative'},\n",
        "\n",
        "    {'role': 'user', 'content': \"It works fine, but nothing special. It's just okay.\"},\n",
        "    {'role': 'assistant', 'content': 'Sentiment: Neutral'},\n",
        "\n",
        "    {'role': 'user', 'content': \"The price is good, but the quality could be better.\"},\n",
        "    {'role': 'assistant', 'content': 'Sentiment: Neutral'},\n",
        "\n",
        "    {'role': 'user', 'content': \"The design is beautiful, but the battery drains too fast.\"},\n",
        "    {'role': 'assistant', 'content': 'Sentiment: Negative'}\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "23a4b145",
      "metadata": {
        "id": "23a4b145"
      },
      "outputs": [],
      "source": [
        "# Test sentences and their ground truth sentiments\n",
        "test_cases = [\n",
        "    {\"sentence\": \"The quality is outstanding, but I had trouble with shipping delays.\", \"ground_truth\": \"Neutral\"},\n",
        "    {\"sentence\": \"It stopped working after just one week! Not worth the money.\", \"ground_truth\": \"Negative\"},\n",
        "    {\"sentence\": \"The design is nice, but the battery drains too fast.\", \"ground_truth\": \"Negative\"},\n",
        "    {\"sentence\": \"It works as expected. Nothing special, but no major complaints.\", \"ground_truth\": \"Neutral\"},\n",
        "    {\"sentence\": \"Fantastic product! The build quality and performance are excellent.\", \"ground_truth\": \"Positive\"}\n",
        "]\n",
        "\n",
        "# Initialize counters for correct predictions\n",
        "correct_zero_shot = 0\n",
        "correct_one_shot = 0\n",
        "correct_few_shot = 0\n",
        "\n",
        "# Total test cases\n",
        "total_cases = len(test_cases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a5fb7677",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5fb7677",
        "outputId": "7d2d0d25-169d-4f2f-8eab-f059f630dde3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Zero-Shot...\n",
            "Input: The quality is outstanding, but I had trouble with shipping delays.\n",
            "Model Output: The review contains both positive and negative sentiments. The first part of the review (\"The quality is outstanding\") indicates a positive sentiment, while the second part (\"but I had trouble with shipping delays\") indicates a negative sentiment. Therefore, the overall sentiment of this review can be classified as Neutral.\n",
            "Ground Truth: Neutral\n",
            "---\n",
            "Input: It stopped working after just one week! Not worth the money.\n",
            "Model Output: Negative\n",
            "Ground Truth: Negative\n",
            "---\n",
            "Input: The design is nice, but the battery drains too fast.\n",
            "Model Output: Negative\n",
            "Ground Truth: Negative\n",
            "---\n",
            "Input: It works as expected. Nothing special, but no major complaints.\n",
            "Model Output: Neutral\n",
            "Ground Truth: Neutral\n",
            "---\n",
            "Input: Fantastic product! The build quality and performance are excellent.\n",
            "Model Output: Positive\n",
            "Ground Truth: Positive\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# Test Zero-Shot\n",
        "print(\"Testing Zero-Shot...\")\n",
        "for test in test_cases:\n",
        "    response = return_OAIResponse(test[\"sentence\"], context_zero_shot)\n",
        "    print(f\"Input: {test['sentence']}\")\n",
        "    print(f\"Model Output: {response}\")\n",
        "    print(f\"Ground Truth: {test['ground_truth']}\")\n",
        "    print(\"---\")\n",
        "    if test[\"ground_truth\"].lower() in response.lower():\n",
        "        correct_zero_shot += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "70a714d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70a714d9",
        "outputId": "77503be6-32d4-4781-d82e-cc64813bd063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing One-Shot...\n",
            "Input: The quality is outstanding, but I had trouble with shipping delays.\n",
            "Model Output: Sentiment: Neutral\n",
            "Ground Truth: Neutral\n",
            "---\n",
            "Input: It stopped working after just one week! Not worth the money.\n",
            "Model Output: Sentiment: Negative\n",
            "Ground Truth: Negative\n",
            "---\n",
            "Input: The design is nice, but the battery drains too fast.\n",
            "Model Output: Sentiment: Negative\n",
            "Ground Truth: Negative\n",
            "---\n",
            "Input: It works as expected. Nothing special, but no major complaints.\n",
            "Model Output: Sentiment: Neutral\n",
            "Ground Truth: Neutral\n",
            "---\n",
            "Input: Fantastic product! The build quality and performance are excellent.\n",
            "Model Output: Sentiment: Positive\n",
            "Ground Truth: Positive\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# Test One-Shot\n",
        "print(\"\\nTesting One-Shot...\")\n",
        "for test in test_cases:\n",
        "    response = return_OAIResponse(test[\"sentence\"], context_one_shot)\n",
        "    print(f\"Input: {test['sentence']}\")\n",
        "    print(f\"Model Output: {response}\")\n",
        "    print(f\"Ground Truth: {test['ground_truth']}\")\n",
        "    print(\"---\")\n",
        "    if test[\"ground_truth\"].lower() in response.lower():\n",
        "        correct_one_shot += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "984f3df5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "984f3df5",
        "outputId": "54891720-3681-43af-ad59-7de770fa27fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing Few-Shot...\n",
            "Input: The quality is outstanding, but I had trouble with shipping delays.\n",
            "Model Output: Sentiment: Neutral\n",
            "Ground Truth: Neutral\n",
            "---\n",
            "Input: It stopped working after just one week! Not worth the money.\n",
            "Model Output: Sentiment: Negative\n",
            "Ground Truth: Negative\n",
            "---\n",
            "Input: The design is nice, but the battery drains too fast.\n",
            "Model Output: Sentiment: Neutral\n",
            "Ground Truth: Negative\n",
            "---\n",
            "Input: It works as expected. Nothing special, but no major complaints.\n",
            "Model Output: Sentiment: Neutral\n",
            "Ground Truth: Neutral\n",
            "---\n",
            "Input: Fantastic product! The build quality and performance are excellent.\n",
            "Model Output: Sentiment: Positive\n",
            "Ground Truth: Positive\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# Test Few-Shot\n",
        "print(\"\\nTesting Few-Shot...\")\n",
        "for test in test_cases:\n",
        "    response = return_OAIResponse(test[\"sentence\"], context_few_shot)\n",
        "    print(f\"Input: {test['sentence']}\")\n",
        "    print(f\"Model Output: {response}\")\n",
        "    print(f\"Ground Truth: {test['ground_truth']}\")\n",
        "    print(\"---\")\n",
        "    if test[\"ground_truth\"].lower() in response.lower():\n",
        "        correct_few_shot += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "17925d50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17925d50",
        "outputId": "bbb4e2fc-aac6-407a-93f9-92722e0ed877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy Results:\n",
            "Zero-Shot: 100.00% (5/5 correct)\n",
            "One-Shot: 100.00% (5/5 correct)\n",
            "Few-Shot: 80.00% (4/5 correct)\n"
          ]
        }
      ],
      "source": [
        "# Calculate and print accuracy for each context\n",
        "accuracy_zero_shot = (correct_zero_shot / total_cases) * 100\n",
        "accuracy_one_shot = (correct_one_shot / total_cases) * 100\n",
        "accuracy_few_shot = (correct_few_shot / total_cases) * 100\n",
        "\n",
        "print(\"\\nAccuracy Results:\")\n",
        "print(f\"Zero-Shot: {accuracy_zero_shot:.2f}% ({correct_zero_shot}/{total_cases} correct)\")\n",
        "print(f\"One-Shot: {accuracy_one_shot:.2f}% ({correct_one_shot}/{total_cases} correct)\")\n",
        "print(f\"Few-Shot: {accuracy_few_shot:.2f}% ({correct_few_shot}/{total_cases} correct)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#📝 **Evaluation of Three Prompting Versions**\n",
        "\n",
        "📌 **Overview**\n",
        "This report evaluates three prompting methods for classifying product reviews: Zero-Shot, One-Shot, and Few-Shot. It focuses on accuracy, clarity, and handling complex sentiments.\n",
        "\n",
        "### Version 1: (Zero-Shot Prompting)\n",
        "\n",
        "✅ **Pros**:\n",
        "\n",
        "* Simple and quick to implement.\n",
        "* No examples required.\n",
        "* Works well for simple tasks.\n",
        "\n",
        "❌ **Cons**:\n",
        "\n",
        "* Struggles with complex or ambiguous sentiments.\n",
        "* Lacks examples to guide the model.\n",
        "\n",
        "📊 **Accuracy**: 80.00% (4/5 correct)\n",
        "\n",
        "### Version 2: (One-Shot Prompting)\n",
        "\n",
        "✅ **Pros**:\n",
        "\n",
        "* Provides a single example to guide the model.\n",
        "* Significantly better accuracy than Zero-Shot.\n",
        "\n",
        "❌ **Cons**:\n",
        "\n",
        "* Limited to one example, which may not cover all cases.\n",
        "\n",
        "📊 **Accuracy**: 100.00% (5/5 correct)\n",
        "\n",
        "### Version 3: (Few-Shot Prompting)\n",
        "\n",
        "✅ **Pros**:\n",
        "\n",
        "* Multiple examples cover various sentiments.\n",
        "* Rich context improves model performance.\n",
        "* Handles complex or ambiguous reviews well.\n",
        "\n",
        "❌ **Cons**:\n",
        "\n",
        "* Requires more setup due to multiple examples.\n",
        "\n",
        "📊 **Accuracy**: 100.00% (5/5 correct)\n",
        "\n",
        "### 🎯 **Conclusion**\n",
        "\n",
        "One-Shot and Few-Shot achieved perfect accuracy (100%), proving their effectiveness. Few-Shot is particularly strong for nuanced or ambiguous reviews. The experiment highlights the importance of context and diverse examples to improve model performance.\n"
      ],
      "metadata": {
        "id": "ZEOlGiVmakBe"
      },
      "id": "ZEOlGiVmakBe"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}