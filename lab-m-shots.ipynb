{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b19fff-8f42-4e9f-a73e-00cff106805a",
   "metadata": {},
   "source": [
    "# M-Shots Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34723a72-1601-4685-a0ba-bff544425d48",
   "metadata": {
    "id": "34723a72-1601-4685-a0ba-bff544425d48"
   },
   "source": [
    "In this notebook, we'll explore small prompt engineering techniques and recommendations that will help us elicit responses from the models that are better suited to our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba193cc-d8a0-4ad2-8177-380204426859",
   "metadata": {
    "id": "fba193cc-d8a0-4ad2-8177-380204426859"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502cfc93-21e0-498f-9650-37bc6ddd514d",
   "metadata": {
    "id": "502cfc93-21e0-498f-9650-37bc6ddd514d"
   },
   "source": [
    "# Formatting the answer with Few Shot Samples.\n",
    "\n",
    "To obtain the model's response in a specific format, we have various options, but one of the most convenient is to use Few-Shot Samples. This involves presenting the model with pairs of user queries and example responses.\n",
    "\n",
    "Large models like GPT-3.5 respond well to the examples provided, adapting their response to the specified format.\n",
    "\n",
    "Depending on the number of examples given, this technique can be referred to as:\n",
    "* Zero-Shot.\n",
    "* One-Shot.\n",
    "* Few-Shots.\n",
    "\n",
    "With One Shot should be enough, and it is recommended to use a maximum of six shots. It's important to remember that this information is passed in each query and occupies space in the input prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8344712-06d7-4c24-83d8-f36d62926e5e",
   "metadata": {
    "id": "a8344712-06d7-4c24-83d8-f36d62926e5e"
   },
   "outputs": [],
   "source": [
    "# Function to call the model.\n",
    "def return_OAIResponse(user_message, context):\n",
    "    client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "    newcontext = context.copy()\n",
    "    newcontext.append({'role':'user', 'content':\"question: \" + user_message})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=newcontext,\n",
    "            temperature=1,\n",
    "        )\n",
    "\n",
    "    return (response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611d73d-9330-466d-b705-543667e1b561",
   "metadata": {
    "id": "f611d73d-9330-466d-b705-543667e1b561"
   },
   "source": [
    "In this zero-shots prompt we obtain a correct response, but without formatting, as the model incorporates the information he wants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
    "outputId": "4c4a9f4f-67c9-4a11-837f-1a1fd6b516ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Vettel won the F1 World Championship in 2010, driving for the Red Bull Racing team. It was his first of four championships during his career.\n"
     ]
    }
   ],
   "source": [
    "#zero-shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are an expert in F1.'}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2010?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8",
   "metadata": {
    "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8"
   },
   "source": [
    "For a model as large and good as GPT 3.5, a single shot is enough to learn the output format we expect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
    "outputId": "5278df23-8797-4dc2-9340-ac29c1318a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2011 F1 World Championship was won by Sebastian Vettel, who drove for the Red Bull Racing Team.\n"
     ]
    }
   ],
   "source": [
    "#one-shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in F1.\n",
    "\n",
    "     Who won the 2000 f1 championship?\n",
    "     Driver: Michael Schumacher.\n",
    "     Team: Ferrari.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2011?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c454a8-181b-482b-873a-81d6ffde4674",
   "metadata": {
    "id": "32c454a8-181b-482b-873a-81d6ffde4674"
   },
   "source": [
    "Smaller models, or more complicated formats, may require more than one shot. Here a sample with two shots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
    "outputId": "a6f90f5d-6d68-4b3d-ccb5-5848ae4e3e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2006 F1 championship was won by Fernando Alonso driving for Renault.\n"
     ]
    }
   ],
   "source": [
    "#Few shots\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in F1.\n",
    "\n",
    "     Who won the 2010 f1 championship?\n",
    "     Driver: Sebastian Vettel.\n",
    "     Team: Red Bull Renault.\n",
    "\n",
    "     Who won the 2009 f1 championship?\n",
    "     Driver: Jenson Button.\n",
    "     Team: BrawnGP.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
    "outputId": "75f63fe3-0efc-45ed-dd45-71dbbb08d7a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Lewis Hamilton.\n",
      "Team: Mercedes.\n"
     ]
    }
   ],
   "source": [
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86",
   "metadata": {
    "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86"
   },
   "source": [
    "We've been creating the prompt without using OpenAI's roles, and as we've seen, it worked correctly.\n",
    "\n",
    "However, the proper way to do this is by using these roles to construct the prompt, making the model's learning process even more effective.\n",
    "\n",
    "By not feeding it the entire prompt as if they were system commands, we enable the model to learn from a conversation, which is more realistic for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
    "outputId": "868d2040-ca3c-4a47-a1e8-1e08d581191d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Lewis Hamilton. \n",
      "Team: Mercedes. \n",
      "Points: 413.\n"
     ]
    }
   ],
   "source": [
    "#Recomended solution\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are and expert in f1.\\n\\n'},\n",
    "    {'role':'user', 'content':'Who won the 2010 f1 championship?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Driver: Sebastian Vettel. \\nTeam: Red Bull. \\nPoints: 256. \"\"\"},\n",
    "    {'role':'user', 'content':'Who won the 2009 f1 championship?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Driver: Jenson Button. \\nTeam: BrawnGP. \\nPoints: 95. \"\"\"},\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f6b42-f351-496b-a7e8-1286426457eb",
   "metadata": {
    "id": "ac6f6b42-f351-496b-a7e8-1286426457eb"
   },
   "source": [
    "We could also address it by using a more conventional prompt, describing what we want and how we want the format.\n",
    "\n",
    "However, it's essential to understand that in this case, the model is following instructions, whereas in the case of use shots, it is learning in real-time during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
    "outputId": "4c970dde-37ff-41a9-8d4e-37bb727f47a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Lewis Hamilton\n",
      "Team: Mercedes\n",
      "Points: 413\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\"\"\"You are and expert in f1.\n",
    "    You are going to answer the question of the user giving the name of the rider,\n",
    "    the name of the team and the points of the champion, following the format:\n",
    "    Drive:\n",
    "    Team:\n",
    "    Points: \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "KNDL1GzVngyL",
   "metadata": {
    "id": "KNDL1GzVngyL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Fernando Alonso.\n",
      "Team: Renault.\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are classifying .\n",
    "\n",
    "     Who won the 2010 f1 championship?\n",
    "     Driver: Sebastian Vettel.\n",
    "     Team: Red Bull Renault.\n",
    "\n",
    "     Who won the 2009 f1 championship?\n",
    "     Driver: Jenson Button.\n",
    "     Team: BrawnGP.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qZPNTLMPnkQ4",
   "metadata": {
    "id": "qZPNTLMPnkQ4"
   },
   "source": [
    "Few Shots for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ejcstgTxnnX5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejcstgTxnnX5",
    "outputId": "4b91cc73-18f6-4944-a46b-806b02b7becb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Neutral\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in reviewing product opinions and classifying them as positive or negative.\n",
    "\n",
    "     It fulfilled its function perfectly, I think the price is fair, I would buy it again.\n",
    "     Sentiment: Positive\n",
    "\n",
    "     It didn't work bad, but I wouldn't buy it again, maybe it's a bit expensive for what it does.\n",
    "     Sentiment: Negative.\n",
    "\n",
    "     I wouldn't know what to say, my son uses it, but he doesn't love it.\n",
    "     Sentiment: Neutral\n",
    "     \"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"I'm not going to return it, but I don't plan to buy it again.\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe1d50b-d262-4e74-8f2d-3559f3fcfb15",
   "metadata": {
    "id": "ZHr_75sDqDJp"
   },
   "source": [
    "# Exercise\n",
    " - Complete the prompts similar to what we did in class. \n",
    "     - Try at least 3 versions\n",
    "     - Be creative\n",
    " - Write a one page report summarizing your findings.\n",
    "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
    " - What did you learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad9bc2",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The Apollo 11 response was well-structured and factual but lacked deeper technical details.\n",
    "\n",
    "The Mars colonization response covered key challenges like radiation, resources, and mental health but didn’t include recent advancements.\n",
    "\n",
    "The moon landing controversy response was clear and logical, providing strong counterarguments, but didn’t analyze why the conspiracy theory persists.\n",
    "\n",
    "### Where GPT Struggled\n",
    "\n",
    "It did not include recent technology updates unless explicitly asked.\n",
    "\n",
    "No hallucinations were found, but some answers could be more detailed.\n",
    "\n",
    "Responses were factual but lacked critical analysis or predictions about the future.\n",
    "\n",
    "### What I Learned\n",
    "\n",
    "GPT-3.5 follows structured prompts well, especially with step-by-step formats.\n",
    "\n",
    "Asking for real-world examples or current research improves responses.\n",
    "\n",
    "Adding context or specific constraints helps refine the output for better insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef3a1b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mission Breakdown Response:\n",
      " 1. **Launch**: The Apollo 11 mission launched from Kennedy Space Center in Florida on July 16, 1969, aboard a Saturn V rocket.\n",
      "\n",
      "2. **Earth Orbit**: After reaching Earth orbit, the spacecraft traveled towards the Moon for about three days.\n",
      "\n",
      "3. **Trans-Lunar Injection**: Once the spacecraft was on the correct trajectory, the third-stage rocket fired again to propel the spacecraft towards the Moon. \n",
      "\n",
      "4. **Approaching the Moon**: The spacecraft entered the Moon's orbit and performed maneuvers to prepare for landing.\n",
      "\n",
      "5. **Lunar Module Separation**: The lunar module, named Eagle, separated from the command module, Columbia, carrying astronauts Neil Armstrong and Edwin \"Buzz\" Aldrin.\n",
      "\n",
      "6. **Descent**: The lunar module began its descent to the Moon's surface, with astronaut Michael Collins remaining in the command module in orbit.\n",
      "\n",
      "7. **Landing**: On July 20, 1969, the lunar module successfully landed on the Moon's surface in the Sea of Tranquility.\n",
      "\n",
      "8. **Neil Armstrong's First Steps**: Neil Armstrong became the first human to step onto the lunar surface, uttering the famous words, \"That's one small step for a man, one giant leap for mankind.\"\n",
      "\n",
      "9. **Lunar Exploration**: Armstrong and Aldrin spent about 2.5 hours conducting experiments, collecting samples, and taking photographs on the lunar surface.\n",
      "\n",
      "10. **Lift-off**: After completing their tasks on the Moon, the astronauts returned to the lunar module, and it lifted off from the surface to rendezvous with the command module.\n",
      "\n",
      "11. **Return to Earth**: The lunar module docked with the command module, and the astronauts transferred back to the Columbia.\n",
      "\n",
      "12. **Splashdown**: On July 24, 1969, the Apollo 11 spacecraft splashed down in the Pacific Ocean, successfully completing the historic mission to land humans on the Moon for the first time.\n",
      "\n",
      "This successful landing not only achieved a historic milestone but also paved the way for further exploration and scientific discoveries in space. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define context for testing new prompts in space exploration\n",
    "context_space = [{'role': 'system', 'content': 'You are a space exploration expert.'}]\n",
    "\n",
    "# Mission Breakdown Prompt\n",
    "context_mission = context_space.copy()\n",
    "context_mission.append({'role': 'user', 'content': \n",
    "    \"Explain in a step-by-step manner how the Apollo 11 mission successfully landed on the Moon.\"})\n",
    "print(\" Mission Breakdown Response:\\n\", return_OAIResponse(\"How did Apollo 11 land on the Moon?\", context_mission), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "967e1001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future Space Exploration Response:\n",
      " Colonizing Mars presents a multitude of challenges, some of which include: \n",
      "\n",
      "1. **Radiation Exposure**: Mars has a very thin atmosphere that offers much less protection from harmful cosmic and solar radiation compared to Earth. Long-term exposure to this radiation can be damaging to human health.\n",
      "\n",
      "2. **Harsh Environment**: Mars' extreme temperatures, low atmospheric pressure, and the presence of toxic soil pose significant challenges for human habitation. Building suitable habitats and infrastructure to support life in such conditions is crucial.\n",
      "\n",
      "3. **Water and Food**: Mars is a barren planet with no readily available sources of water or food. Finding ways to extract and utilize water from Martian resources, as well as establishing sustainable food production systems, will be critical to long-term survival.\n",
      "\n",
      "4. **Supply Chain Reliability**: The distance between Mars and Earth means that supply missions will be infrequent and potentially unreliable. Developing a self-sufficient economy and infrastructure on Mars will be essential to mitigate the risks associated with relying on external support.\n",
      "\n",
      "5. **Mental and Physical Health**: Living in isolation and confinement on Mars for extended periods can have significant psychological and physiological effects on astronauts. Maintaining mental well-being and physical health through appropriate support systems and countermeasures will be crucial.\n",
      "\n",
      "6. **Long-Term Sustainability**: Ensuring the long-term sustainability of a Martian colony, including managing resources, waste, and environmental impacts, will be essential for the survival and prosperity of future generations on the red planet.\n",
      "\n",
      "Addressing these challenges will require significant technological advancements, international collaboration, and a deep understanding of the Martian environment. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Future Space Exploration Prompt\n",
    "context_future = context_space.copy()\n",
    "context_future.append({'role': 'user', 'content': \n",
    "    \"What are the biggest challenges humans will face when colonizing Mars?\"})\n",
    "print(\"Future Space Exploration Response:\\n\", return_OAIResponse(\"What are the biggest challenges of colonizing Mars?\", context_future), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "953b3381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Space Controversy Response:\n",
      " One of the most debated controversies in space exploration history is whether or not the United States' Apollo moon landings were real or if they were staged. Some conspiracy theorists believe that the moon landings were faked by NASA in order to win the Space Race against the Soviet Union during the Cold War. Despite overwhelming evidence supporting the authenticity of the moon landings, including samples of moon rocks brought back to Earth, photographs, and testimonies from astronauts and scientists involved in the missions, the conspiracy theories persist and continue to spark debates among skeptics and supporters alike. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Space Controversies Prompt\n",
    "context_controversy_space = context_space.copy()\n",
    "context_controversy_space.append({'role': 'user', 'content': \n",
    "    \"What is the most debated controversy in space exploration history?\"})\n",
    "print(\"Space Controversy Response:\\n\", return_OAIResponse(\"What is the biggest space exploration controversy?\", context_controversy_space), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3513a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
