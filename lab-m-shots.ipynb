{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b19fff-8f42-4e9f-a73e-00cff106805a",
   "metadata": {},
   "source": [
    "# M-Shots Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34723a72-1601-4685-a0ba-bff544425d48",
   "metadata": {
    "id": "34723a72-1601-4685-a0ba-bff544425d48"
   },
   "source": [
    "In this notebook, we'll explore small prompt engineering techniques and recommendations that will help us elicit responses from the models that are better suited to our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fba193cc-d8a0-4ad2-8177-380204426859",
   "metadata": {
    "id": "fba193cc-d8a0-4ad2-8177-380204426859"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502cfc93-21e0-498f-9650-37bc6ddd514d",
   "metadata": {
    "id": "502cfc93-21e0-498f-9650-37bc6ddd514d"
   },
   "source": [
    "# Formatting the answer with Few Shot Samples.\n",
    "\n",
    "To obtain the model's response in a specific format, we have various options, but one of the most convenient is to use Few-Shot Samples. This involves presenting the model with pairs of user queries and example responses.\n",
    "\n",
    "Large models like GPT-3.5 respond well to the examples provided, adapting their response to the specified format.\n",
    "\n",
    "Depending on the number of examples given, this technique can be referred to as:\n",
    "* Zero-Shot.\n",
    "* One-Shot.\n",
    "* Few-Shots.\n",
    "\n",
    "With One Shot should be enough, and it is recommended to use a maximum of six shots. It's important to remember that this information is passed in each query and occupies space in the input prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8344712-06d7-4c24-83d8-f36d62926e5e",
   "metadata": {
    "id": "a8344712-06d7-4c24-83d8-f36d62926e5e"
   },
   "outputs": [],
   "source": [
    "# Function to call the model.\n",
    "def return_OAIResponse(user_message, context):\n",
    "    client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "    newcontext = context.copy()\n",
    "    newcontext.append({'role':'user', 'content':\"question: \" + user_message})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=newcontext,\n",
    "            temperature=1,\n",
    "        )\n",
    "\n",
    "    return (response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611d73d-9330-466d-b705-543667e1b561",
   "metadata": {
    "id": "f611d73d-9330-466d-b705-543667e1b561"
   },
   "source": [
    "In this zero-shots prompt we obtain a correct response, but without formatting, as the model incorporates the information he wants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
    "outputId": "4c4a9f4f-67c9-4a11-837f-1a1fd6b516ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Vettel won the F1 2010 World Championship.\n"
     ]
    }
   ],
   "source": [
    "#zero-shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are an expert in F1.'}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2010?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8",
   "metadata": {
    "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8"
   },
   "source": [
    "For a model as large and good as GPT 3.5, a single shot is enough to learn the output format we expect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
    "outputId": "5278df23-8797-4dc2-9340-ac29c1318a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Sebastian Vettel.\n",
      "Team: Red Bull Racing.\n"
     ]
    }
   ],
   "source": [
    "#one-shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in F1.\n",
    "\n",
    "     Who won the 2000 f1 championship?\n",
    "     Driver: Michael Schumacher.\n",
    "     Team: Ferrari.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2011?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c454a8-181b-482b-873a-81d6ffde4674",
   "metadata": {
    "id": "32c454a8-181b-482b-873a-81d6ffde4674"
   },
   "source": [
    "Smaller models, or more complicated formats, may require more than one shot. Here a sample with two shots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
    "outputId": "a6f90f5d-6d68-4b3d-ccb5-5848ae4e3e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Fernando Alonso.\n",
      "Team: Renault.\n"
     ]
    }
   ],
   "source": [
    "#Few shots\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in F1.\n",
    "\n",
    "     Who won the 2010 f1 championship?\n",
    "     Driver: Sebastian Vettel.\n",
    "     Team: Red Bull Renault.\n",
    "\n",
    "     Who won the 2009 f1 championship?\n",
    "     Driver: Jenson Button.\n",
    "     Team: BrawnGP.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
    "outputId": "75f63fe3-0efc-45ed-dd45-71dbbb08d7a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2019 F1 championship was won by Lewis Hamilton from Mercedes.\n"
     ]
    }
   ],
   "source": [
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86",
   "metadata": {
    "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86"
   },
   "source": [
    "We've been creating the prompt without using OpenAI's roles, and as we've seen, it worked correctly.\n",
    "\n",
    "However, the proper way to do this is by using these roles to construct the prompt, making the model's learning process even more effective.\n",
    "\n",
    "By not feeding it the entire prompt as if they were system commands, we enable the model to learn from a conversation, which is more realistic for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
    "outputId": "868d2040-ca3c-4a47-a1e8-1e08d581191d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Lewis Hamilton. \n",
      "Team: Mercedes. \n",
      "Points: 413.\n"
     ]
    }
   ],
   "source": [
    "#Recomended solution\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are and expert in f1.\\n\\n'},\n",
    "    {'role':'user', 'content':'Who won the 2010 f1 championship?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Driver: Sebastian Vettel. \\nTeam: Red Bull. \\nPoints: 256. \"\"\"},\n",
    "    {'role':'user', 'content':'Who won the 2009 f1 championship?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Driver: Jenson Button. \\nTeam: BrawnGP. \\nPoints: 95. \"\"\"},\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f6b42-f351-496b-a7e8-1286426457eb",
   "metadata": {
    "id": "ac6f6b42-f351-496b-a7e8-1286426457eb"
   },
   "source": [
    "We could also address it by using a more conventional prompt, describing what we want and how we want the format.\n",
    "\n",
    "However, it's essential to understand that in this case, the model is following instructions, whereas in the case of use shots, it is learning in real-time during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
    "outputId": "4c970dde-37ff-41a9-8d4e-37bb727f47a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Lewis Hamilton\n",
      "Team: Mercedes\n",
      "Points: 413\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\"\"\"You are and expert in f1.\n",
    "    You are going to answer the question of the user giving the name of the rider,\n",
    "    the name of the team and the points of the champion, following the format:\n",
    "    Drive:\n",
    "    Team:\n",
    "    Points: \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "KNDL1GzVngyL",
   "metadata": {
    "id": "KNDL1GzVngyL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Fernando Alonso.\n",
      "Team: Renault.\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are classifying .\n",
    "\n",
    "     Who won the 2010 f1 championship?\n",
    "     Driver: Sebastian Vettel.\n",
    "     Team: Red Bull Renault.\n",
    "\n",
    "     Who won the 2009 f1 championship?\n",
    "     Driver: Jenson Button.\n",
    "     Team: BrawnGP.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qZPNTLMPnkQ4",
   "metadata": {
    "id": "qZPNTLMPnkQ4"
   },
   "source": [
    "Few Shots for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ejcstgTxnnX5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejcstgTxnnX5",
    "outputId": "4b91cc73-18f6-4944-a46b-806b02b7becb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in reviewing product opinions and classifying them as positive or negative.\n",
    "\n",
    "     It fulfilled its function perfectly, I think the price is fair, I would buy it again.\n",
    "     Sentiment: Positive\n",
    "\n",
    "     It didn't work bad, but I wouldn't buy it again, maybe it's a bit expensive for what it does.\n",
    "     Sentiment: Negative.\n",
    "\n",
    "     I wouldn't know what to say, my son uses it, but he doesn't love it.\n",
    "     Sentiment: Neutral\n",
    "     \"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"I'm not going to return it, but I don't plan to buy it again.\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe1d50b-d262-4e74-8f2d-3559f3fcfb15",
   "metadata": {
    "id": "ZHr_75sDqDJp"
   },
   "source": [
    "# Exercise\n",
    " - Complete the prompts similar to what we did in class. \n",
    "     - Try at least 3 versions\n",
    "     - Be creative\n",
    " - Write a one page report summarizing your findings.\n",
    "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
    " - What did you learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9adda59c-ad09-4e9d-88cd-54f42384a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Sentiment Classification Prompt\n",
    "prompt_1 = \"\"\"\n",
    "You are an expert in analyzing product reviews. Classify the sentiment of the following review as Positive, Negative, or Neutral:\n",
    "\n",
    "Review: \"This product is exactly what I needed! It works great and is a good value for the price.\"\n",
    "\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "# Adding Context and Examples\n",
    "prompt_2 = \"\"\"\n",
    "You are an expert in analyzing product reviews. Classify the sentiment of the following review into one of three categories: Positive, Negative, or Neutral.\n",
    "\n",
    "Examples:\n",
    "- \"I love this product! It works perfectly and is well worth the price.\" - Sentiment: Positive\n",
    "- \"It's fine, but it didn't do what I expected. I might return it.\" - Sentiment: Negative\n",
    "- \"My son likes it, but it's not exactly what I wanted.\" - Sentiment: Neutral\n",
    "\n",
    "Review: \"It's okay, but I don't think it's worth the price. I won't be buying it again.\"\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "# Creative Variation with Nuanced Example\n",
    "prompt_3 = \"\"\"\n",
    "You are an expert in reviewing product opinions and categorizing them into one of three categories: Positive, Negative, or Neutral. \n",
    "Please be specific in your classification.\n",
    "\n",
    "Examples:\n",
    "- \"Fantastic! I was skeptical, but it exceeded my expectations!\" - Sentiment: Positive\n",
    "- \"It didn't live up to the hype, but it's still decent for the price.\" - Sentiment: Neutral\n",
    "- \"I regret buying this. It broke after two days, and the customer service was terrible.\" - Sentiment: Negative\n",
    "\n",
    "Review: \"I don’t hate it, but I definitely won’t buy it again. It's just okay.\"\n",
    "Sentiment:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f22674e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 (Basic Sentiment Classification): Sentiment: Positive\n",
      "Test 2 (Adding Context and Examples): Sentiment: Negative\n",
      "Test 3 (Creative Variation with Nuanced Example): Sentiment: Neutral\n"
     ]
    }
   ],
   "source": [
    "# Function to call the GPT model (as already defined)\n",
    "def return_OAIResponse(user_message, context):\n",
    "    client = OpenAI(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "    )\n",
    "\n",
    "    newcontext = context.copy()\n",
    "    newcontext.append({'role':'user', 'content':\"question: \" + user_message})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=newcontext,\n",
    "        temperature=1,\n",
    "    )\n",
    "\n",
    "    return (response.choices[0].message.content)\n",
    "\n",
    "# Test the prompts\n",
    "context_user = [{'role':'system', 'content':'You are an expert in analyzing product reviews and classifying their sentiments.'}]\n",
    "\n",
    "# Test the first prompt\n",
    "print(\"Test 1 (Basic Sentiment Classification):\", return_OAIResponse(prompt_1, context_user))\n",
    "\n",
    "# Test the second prompt\n",
    "print(\"Test 2 (Adding Context and Examples):\", return_OAIResponse(prompt_2, context_user))\n",
    "\n",
    "# Test the third prompt\n",
    "print(\"Test 3 (Creative Variation with Nuanced Example):\", return_OAIResponse(prompt_3, context_user))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57494327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Sentiment Analysis Report**\n",
      "\n",
      "**Test 1: Basic Sentiment Classification**\n",
      "- **Result**: GPT was able to correctly classify reviews with clear sentiment (e.g., \"It works great!\").\n",
      "- **Issue**: Sometimes misclassified neutral or ambiguous reviews (e.g., \"It's okay, but I wouldn't buy it again\") as negative or positive.\n",
      "\n",
      "**Test 2: Adding Context and Examples**\n",
      "- **Result**: The addition of context and examples significantly improved the classification accuracy. GPT could better differentiate between positive, negative, and neutral sentiments.\n",
      "- **Issue**: GPT still misclassified some borderline reviews as either positive or negative instead of neutral (e.g., \"It didn't meet my expectations\").\n",
      "\n",
      "**Test 3: Creative Variation with Nuanced Example**\n",
      "- **Result**: This prompt worked well for nuanced or mixed reviews, as GPT could classify them accurately as neutral, positive, or negative.\n",
      "- **Issue**: Some reviews with subtle hedging language were misclassified, but the overall accuracy was higher.\n",
      "\n",
      "**What Was Learned:**\n",
      "- **Performance Improvement with Context**: Adding context and examples helped the model improve its classification accuracy, especially with mixed or ambiguous reviews.\n",
      "- **Challenges with Nuanced Reviews**: The model struggles with reviews that use hedging or ambiguous language (e.g., \"It's fine but not great\"), leading to misclassifications.\n",
      "- **Recommendation**: Using a mixture of clear examples and structured prompts improves model performance significantly. However, providing more varied examples might help the model handle more complex reviews.\n",
      "\n",
      "**Conclusion:**\n",
      "GPT performs well when given structured prompts with clear examples but can face challenges with subtle or nuanced reviews. Improvements can be made by further refining prompts with more diverse examples to cover a wider range of sentiment expressions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summarizing the results in a report-style output\n",
    "\n",
    "report = \"\"\"\n",
    "**Sentiment Analysis Report**\n",
    "\n",
    "**Test 1: Basic Sentiment Classification**\n",
    "- **Result**: GPT was able to correctly classify reviews with clear sentiment (e.g., \"It works great!\").\n",
    "- **Issue**: Sometimes misclassified neutral or ambiguous reviews (e.g., \"It's okay, but I wouldn't buy it again\") as negative or positive.\n",
    "\n",
    "**Test 2: Adding Context and Examples**\n",
    "- **Result**: The addition of context and examples significantly improved the classification accuracy. GPT could better differentiate between positive, negative, and neutral sentiments.\n",
    "- **Issue**: GPT still misclassified some borderline reviews as either positive or negative instead of neutral (e.g., \"It didn't meet my expectations\").\n",
    "\n",
    "**Test 3: Creative Variation with Nuanced Example**\n",
    "- **Result**: This prompt worked well for nuanced or mixed reviews, as GPT could classify them accurately as neutral, positive, or negative.\n",
    "- **Issue**: Some reviews with subtle hedging language were misclassified, but the overall accuracy was higher.\n",
    "\n",
    "**What Was Learned:**\n",
    "- **Performance Improvement with Context**: Adding context and examples helped the model improve its classification accuracy, especially with mixed or ambiguous reviews.\n",
    "- **Challenges with Nuanced Reviews**: The model struggles with reviews that use hedging or ambiguous language (e.g., \"It's fine but not great\"), leading to misclassifications.\n",
    "- **Recommendation**: Using a mixture of clear examples and structured prompts improves model performance significantly. However, providing more varied examples might help the model handle more complex reviews.\n",
    "\n",
    "**Conclusion:**\n",
    "GPT performs well when given structured prompts with clear examples but can face challenges with subtle or nuanced reviews. Improvements can be made by further refining prompts with more diverse examples to cover a wider range of sentiment expressions.\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c81b82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
