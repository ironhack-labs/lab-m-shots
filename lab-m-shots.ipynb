{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b19fff-8f42-4e9f-a73e-00cff106805a",
   "metadata": {},
   "source": [
    "# M-Shots Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34723a72-1601-4685-a0ba-bff544425d48",
   "metadata": {
    "id": "34723a72-1601-4685-a0ba-bff544425d48"
   },
   "source": [
    "In this notebook, we'll explore small prompt engineering techniques and recommendations that will help us elicit responses from the models that are better suited to our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fba193cc-d8a0-4ad2-8177-380204426859",
   "metadata": {
    "id": "fba193cc-d8a0-4ad2-8177-380204426859"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d8c83fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502cfc93-21e0-498f-9650-37bc6ddd514d",
   "metadata": {
    "id": "502cfc93-21e0-498f-9650-37bc6ddd514d"
   },
   "source": [
    "# Formatting the answer with Few Shot Samples.\n",
    "\n",
    "To obtain the model's response in a specific format, we have various options, but one of the most convenient is to use Few-Shot Samples. This involves presenting the model with pairs of user queries and example responses.\n",
    "\n",
    "Large models like GPT-3.5 respond well to the examples provided, adapting their response to the specified format.\n",
    "\n",
    "Depending on the number of examples given, this technique can be referred to as:\n",
    "* Zero-Shot.\n",
    "* One-Shot.\n",
    "* Few-Shots.\n",
    "\n",
    "With One Shot should be enough, and it is recommended to use a maximum of six shots. It's important to remember that this information is passed in each query and occupies space in the input prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8344712-06d7-4c24-83d8-f36d62926e5e",
   "metadata": {
    "id": "a8344712-06d7-4c24-83d8-f36d62926e5e"
   },
   "outputs": [],
   "source": [
    "# Function to call the model.\n",
    "def return_OAIResponse(user_message, context):\n",
    "    client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "    newcontext = context.copy()\n",
    "    newcontext.append({'role':'user', 'content':\"question: \" + user_message})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=newcontext,\n",
    "            temperature=1,\n",
    "        )\n",
    "\n",
    "    return (response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611d73d-9330-466d-b705-543667e1b561",
   "metadata": {
    "id": "f611d73d-9330-466d-b705-543667e1b561"
   },
   "source": [
    "In this zero-shots prompt we obtain a correct response, but without formatting, as the model incorporates the information he wants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
    "outputId": "4c4a9f4f-67c9-4a11-837f-1a1fd6b516ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Vettel won the F1 2010 championship driving for Red Bull Racing.\n"
     ]
    }
   ],
   "source": [
    "#zero-shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are an expert in F1.'}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2010?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8",
   "metadata": {
    "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8"
   },
   "source": [
    "For a model as large and good as GPT 3.5, a single shot is enough to learn the output format we expect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
    "outputId": "5278df23-8797-4dc2-9340-ac29c1318a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Sebastian Vettel.\n",
      "Team: Red Bull Racing.\n"
     ]
    }
   ],
   "source": [
    "#one-shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in F1.\n",
    "\n",
    "     Who won the 2000 f1 championship?\n",
    "     Driver: Michael Schumacher.\n",
    "     Team: Ferrari.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2011?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c454a8-181b-482b-873a-81d6ffde4674",
   "metadata": {
    "id": "32c454a8-181b-482b-873a-81d6ffde4674"
   },
   "source": [
    "Smaller models, or more complicated formats, may require more than one shot. Here a sample with two shots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
    "outputId": "a6f90f5d-6d68-4b3d-ccb5-5848ae4e3e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Fernando Alonso.\n",
      "Team: Renault.\n"
     ]
    }
   ],
   "source": [
    "#Few shots\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in F1.\n",
    "\n",
    "     Who won the 2010 f1 championship?\n",
    "     Driver: Sebastian Vettel.\n",
    "     Team: Red Bull Renault.\n",
    "\n",
    "     Who won the 2009 f1 championship?\n",
    "     Driver: Jenson Button.\n",
    "     Team: BrawnGP.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
    "outputId": "75f63fe3-0efc-45ed-dd45-71dbbb08d7a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Lewis Hamilton.\n",
      "Team: Mercedes.\n"
     ]
    }
   ],
   "source": [
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86",
   "metadata": {
    "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86"
   },
   "source": [
    "We've been creating the prompt without using OpenAI's roles, and as we've seen, it worked correctly.\n",
    "\n",
    "However, the proper way to do this is by using these roles to construct the prompt, making the model's learning process even more effective.\n",
    "\n",
    "By not feeding it the entire prompt as if they were system commands, we enable the model to learn from a conversation, which is more realistic for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
    "outputId": "868d2040-ca3c-4a47-a1e8-1e08d581191d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Lewis Hamilton.\n",
      "Team: Mercedes.\n",
      "Points: 413.\n"
     ]
    }
   ],
   "source": [
    "#Recomended solution\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are and expert in f1.\\n\\n'},\n",
    "    {'role':'user', 'content':'Who won the 2010 f1 championship?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Driver: Sebastian Vettel. \\nTeam: Red Bull. \\nPoints: 256. \"\"\"},\n",
    "    {'role':'user', 'content':'Who won the 2009 f1 championship?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Driver: Jenson Button. \\nTeam: BrawnGP. \\nPoints: 95. \"\"\"},\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f6b42-f351-496b-a7e8-1286426457eb",
   "metadata": {
    "id": "ac6f6b42-f351-496b-a7e8-1286426457eb"
   },
   "source": [
    "We could also address it by using a more conventional prompt, describing what we want and how we want the format.\n",
    "\n",
    "However, it's essential to understand that in this case, the model is following instructions, whereas in the case of use shots, it is learning in real-time during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
    "outputId": "4c970dde-37ff-41a9-8d4e-37bb727f47a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Lewis Hamilton\n",
      "Team: Mercedes\n",
      "Points: 413\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\"\"\"You are and expert in f1.\n",
    "    You are going to answer the question of the user giving the name of the rider,\n",
    "    the name of the team and the points of the champion, following the format:\n",
    "    Drive:\n",
    "    Team:\n",
    "    Points: \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "KNDL1GzVngyL",
   "metadata": {
    "id": "KNDL1GzVngyL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Fernando Alonso.\n",
      "Team: Renault.\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are classifying .\n",
    "\n",
    "     Who won the 2010 f1 championship?\n",
    "     Driver: Sebastian Vettel.\n",
    "     Team: Red Bull Renault.\n",
    "\n",
    "     Who won the 2009 f1 championship?\n",
    "     Driver: Jenson Button.\n",
    "     Team: BrawnGP.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qZPNTLMPnkQ4",
   "metadata": {
    "id": "qZPNTLMPnkQ4"
   },
   "source": [
    "Few Shots for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ejcstgTxnnX5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejcstgTxnnX5",
    "outputId": "4b91cc73-18f6-4944-a46b-806b02b7becb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in reviewing product opinions and classifying them as positive or negative.\n",
    "\n",
    "     It fulfilled its function perfectly, I think the price is fair, I would buy it again.\n",
    "     Sentiment: Positive\n",
    "\n",
    "     It didn't work bad, but I wouldn't buy it again, maybe it's a bit expensive for what it does.\n",
    "     Sentiment: Negative.\n",
    "\n",
    "     I wouldn't know what to say, my son uses it, but he doesn't love it.\n",
    "     Sentiment: Neutral\n",
    "     \"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"I'm not going to return it, but I don't plan to buy it again.\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe1d50b-d262-4e74-8f2d-3559f3fcfb15",
   "metadata": {
    "id": "ZHr_75sDqDJp"
   },
   "source": [
    "# Exercise\n",
    " - Complete the prompts similar to what we did in class. \n",
    "     - Try at least 3 versions\n",
    "     - Be creative\n",
    " - Write a one page report summarizing your findings.\n",
    "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
    " - What did you learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9adda59c-ad09-4e9d-88cd-54f42384a5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whether to keep your horse barefoot or use horseshoes ultimately depends on several factors such as your horse's individual needs, the terrain they are in, their workload, and any existing hoof issues.\n",
      "\n",
      "Barefoot horses can be beneficial as it allows their hooves to naturally expand and contract, promoting better circulation and overall hoof health. It can also help improve their balance and coordination. However, not all horses can go barefoot comfortably, especially if they have brittle hooves or are working on rough, hard surfaces.\n",
      "\n",
      "If your horse is experiencing hoof issues or discomfort, using horseshoes may provide additional support and protection. Horseshoes can help prevent excessive wear and tear on the hooves, especially for horses in heavy work or those with hoof conformation issues.\n",
      "\n",
      "It's best to consult with your farrier or veterinarian to determine the best option for your horse based on their individual needs and circumstances. They can assess your horse's hooves and provide recommendations on whether going barefoot or using horseshoes would be more suitable.\n"
     ]
    }
   ],
   "source": [
    "# Zero shot prompt\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert on equestrian topics and your job is to answer questions for horse owners on an online forum.\n",
    "     \"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Should I let my horse go barefoot or are normal horseshoes better?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "393b4680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you suspect that your horse may have ingested a poisonous plant, it is important to act quickly and seek veterinary assistance immediately. Some common signs of plant poisoning in horses include colic, diarrhea, difficulty breathing, staggering, trembling, or seizures. \n",
      "\n",
      "In the meantime, try to remove your horse from the pasture or area where the suspected plant is located to prevent further ingestion. Offer clean, fresh water and do not give your horse any food until a veterinarian can evaluate the situation. \n",
      "\n",
      "When you contact the vet, provide as much information as you can about the plant in question, including its appearance and any recent changes in your horse's behavior or health. The vet may recommend specific treatment based on the type of plant that was ingested. Immediate veterinary care is crucial in cases of plant poisoning, so do not hesitate to seek help as soon as possible.\n"
     ]
    }
   ],
   "source": [
    "print(return_OAIResponse(\"Help! My horse is feeling sick! I am scared that it might have eaten a poisonous plant. What must I do?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "012a49e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 4: If you have 3 apples and you eat 2, how many apples do you have left?\n",
      "Answer: 1\n",
      "\n",
      "Question 5: What is the next number in this sequence: 2, 4, 6, 8, __\n",
      "Answer: 10\n",
      "\n",
      "Question 6: If a rectangle has a length of 5 cm and a width of 3 cm, what is its area?\n",
      "Answer: 15 square cm\n",
      "\n",
      "Question 7: How many sides does a triangle have?\n",
      "Answer: 3\n",
      "\n",
      "Question 8: If you add 7 to a number and then subtract 3, the result is 12. What is the original number?\n",
      "Answer: 8\n",
      "\n",
      "Question 9: What is the value of 5 squared?\n",
      "Answer: 25\n",
      "\n",
      "Question 10: If a box contains 20 pencils and you take out 5, how many pencils are left in the box?\n",
      "Answer: 15\n"
     ]
    }
   ],
   "source": [
    "# Few shot prompt\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are a primary school teacher, creating questions for a math test.\n",
    "     Question 1: 4+5\n",
    "     Answer: 9\n",
    "\n",
    "     Question 2: 10 x 3\n",
    "     Answer: 30\n",
    "\n",
    "    Question 3: 8 / 2\n",
    "    Answer: 4\n",
    "     \"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Please come up with 10 questions\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f9a26d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 4: 7 + 3\n",
      "\n",
      "Question 5: 6 x 4\n",
      "\n",
      "Question 6: 15 / 3\n",
      "\n",
      "Question 7: 9 - 2\n",
      "\n",
      "Question 8: 12 + 8\n",
      "\n",
      "Question 9: 5 x 7\n",
      "\n",
      "Question 10: 20 / 4\n",
      "\n",
      "Question 11: 14 - 6\n",
      "\n",
      "Question 12: 3 + 9\n",
      "\n",
      "Question 13: 11 x 2\n"
     ]
    }
   ],
   "source": [
    "# Few shot prompt, revised\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are a primary school teacher, creating questions for a math test.\n",
    "     Question 1: 4+5\n",
    "     Answer: 9\n",
    "\n",
    "     Question 2: 10 x 3\n",
    "     Answer: 30\n",
    "\n",
    "    Question 3: 8 / 2\n",
    "    Answer: 4\n",
    "     \"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Please come up with 10 more questions, which are similar to the 3 examples provided \", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e961cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 4: 7 - 3    \n",
      "Answer: 4\n",
      "\n",
      "Question 5: 6 x 2    \n",
      "Answer: 12\n",
      "\n",
      "Question 6: 15 / 3    \n",
      "Answer: 5\n",
      "\n",
      "Question 7: 9 + 6    \n",
      "Answer: 15\n",
      "\n",
      "Question 8: 20 - 13     \n",
      "Answer: 7\n",
      "\n",
      "Question 9: 4 x 4    \n",
      "Answer: 16\n",
      "\n",
      "Question 10: 18 / 2    \n",
      "Answer: 9\n",
      "\n",
      "Question 11: 12 + 3    \n",
      "Answer: 15\n",
      "\n",
      "Question 12: 50 - 20    \n",
      "Answer: 30\n",
      "\n",
      "Question 13: 9 x 3    \n",
      "Answer: 27\n"
     ]
    }
   ],
   "source": [
    "# Few shot prompt - role \"assistant\"\n",
    "context_user = [\n",
    "    {'role':'assistant', 'content':\n",
    "     \"\"\"You are a primary school teacher, creating questions for a math test.\n",
    "     Question 1: 4+5\n",
    "     Answer: 9\n",
    "\n",
    "     Question 2: 10 x 3\n",
    "     Answer: 30\n",
    "\n",
    "    Question 3: 8 / 2\n",
    "    Answer: 4\n",
    "     \"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Please come up with 10 more questions like the examples\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12e93b3",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- I tried a more creative example of an equestrian expert. Without giving many instructions, the model was able to give great answers\n",
    "- I tried an example of a maths teacher setting up a test. Here, the model did not perform so well. I gave 3 example questions and asked for 10. The model only gave the remaining 7 instead of 10 more and the aquestions did not match the examples provided at all\n",
    "- When I asked for the questions to be similar to the examples, the model only gave me the questions and not \"Question: ... Answer: ...\" as in the examples\n",
    "- When I changed the role from \"system\" to \"assistant\", the answer was perfect\n",
    "- I learnt that on more specific tasks, we have to be more specific in the prompts as well\n",
    "- Also, the role makes a difference in how the model \"thinks\" and answers"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
