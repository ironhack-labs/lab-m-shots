{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b19fff-8f42-4e9f-a73e-00cff106805a",
   "metadata": {},
   "source": [
    "# M-Shots Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34723a72-1601-4685-a0ba-bff544425d48",
   "metadata": {
    "id": "34723a72-1601-4685-a0ba-bff544425d48"
   },
   "source": [
    "In this notebook, we'll explore small prompt engineering techniques and recommendations that will help us elicit responses from the models that are better suited to our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba193cc-d8a0-4ad2-8177-380204426859",
   "metadata": {
    "id": "fba193cc-d8a0-4ad2-8177-380204426859"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502cfc93-21e0-498f-9650-37bc6ddd514d",
   "metadata": {
    "id": "502cfc93-21e0-498f-9650-37bc6ddd514d"
   },
   "source": [
    "# Formatting the answer with Few Shot Samples.\n",
    "\n",
    "To obtain the model's response in a specific format, we have various options, but one of the most convenient is to use Few-Shot Samples. This involves presenting the model with pairs of user queries and example responses.\n",
    "\n",
    "Large models like GPT-3.5 respond well to the examples provided, adapting their response to the specified format.\n",
    "\n",
    "Depending on the number of examples given, this technique can be referred to as:\n",
    "* Zero-Shot.\n",
    "* One-Shot.\n",
    "* Few-Shots.\n",
    "\n",
    "With One Shot should be enough, and it is recommended to use a maximum of six shots. It's important to remember that this information is passed in each query and occupies space in the input prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8344712-06d7-4c24-83d8-f36d62926e5e",
   "metadata": {
    "id": "a8344712-06d7-4c24-83d8-f36d62926e5e"
   },
   "outputs": [],
   "source": [
    "# Function to call the model.\n",
    "def return_OAIResponse(user_message, context):\n",
    "    client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "    newcontext = context.copy()\n",
    "    newcontext.append({'role':'user', 'content':\"question: \" + user_message})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=newcontext,\n",
    "            temperature=1,\n",
    "        )\n",
    "\n",
    "    return (response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611d73d-9330-466d-b705-543667e1b561",
   "metadata": {
    "id": "f611d73d-9330-466d-b705-543667e1b561"
   },
   "source": [
    "In this zero-shots prompt we obtain a correct response, but without formatting, as the model incorporates the information he wants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
    "outputId": "4c4a9f4f-67c9-4a11-837f-1a1fd6b516ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2010 Formula 1 World Championship was won by Sebastian Vettel, driving for Red Bull Racing.\n"
     ]
    }
   ],
   "source": [
    "#zero-shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are an expert in F1.'}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2010?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8",
   "metadata": {
    "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8"
   },
   "source": [
    "For a model as large and good as GPT 3.5, a single shot is enough to learn the output format we expect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
    "outputId": "5278df23-8797-4dc2-9340-ac29c1318a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Sebastian Vettel.\n",
      "Team: Red Bull Racing.\n"
     ]
    }
   ],
   "source": [
    "#one-shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in F1.\n",
    "\n",
    "     Who won the 2000 f1 championship?\n",
    "     Driver: Michael Schumacher.\n",
    "     Team: Ferrari.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2011?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c454a8-181b-482b-873a-81d6ffde4674",
   "metadata": {
    "id": "32c454a8-181b-482b-873a-81d6ffde4674"
   },
   "source": [
    "Smaller models, or more complicated formats, may require more than one shot. Here a sample with two shots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
    "outputId": "a6f90f5d-6d68-4b3d-ccb5-5848ae4e3e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2006 F1 championship was won by Fernando Alonso, driving for Renault.\n"
     ]
    }
   ],
   "source": [
    "#Few shots\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in F1.\n",
    "\n",
    "     Who won the 2010 f1 championship?\n",
    "     Driver: Sebastian Vettel.\n",
    "     Team: Red Bull Renault.\n",
    "\n",
    "     Who won the 2009 f1 championship?\n",
    "     Driver: Jenson Button.\n",
    "     Team: BrawnGP.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
    "outputId": "75f63fe3-0efc-45ed-dd45-71dbbb08d7a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2019 F1 Championship was won by Lewis Hamilton driving for Mercedes.\n"
     ]
    }
   ],
   "source": [
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86",
   "metadata": {
    "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86"
   },
   "source": [
    "We've been creating the prompt without using OpenAI's roles, and as we've seen, it worked correctly.\n",
    "\n",
    "However, the proper way to do this is by using these roles to construct the prompt, making the model's learning process even more effective.\n",
    "\n",
    "By not feeding it the entire prompt as if they were system commands, we enable the model to learn from a conversation, which is more realistic for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
    "outputId": "868d2040-ca3c-4a47-a1e8-1e08d581191d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Lewis Hamilton. \n",
      "Team: Mercedes. \n",
      "Points: 413.\n"
     ]
    }
   ],
   "source": [
    "#Recomended solution\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are and expert in f1.\\n\\n'},\n",
    "    {'role':'user', 'content':'Who won the 2010 f1 championship?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Driver: Sebastian Vettel. \\nTeam: Red Bull. \\nPoints: 256. \"\"\"},\n",
    "    {'role':'user', 'content':'Who won the 2009 f1 championship?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Driver: Jenson Button. \\nTeam: BrawnGP. \\nPoints: 95. \"\"\"},\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f6b42-f351-496b-a7e8-1286426457eb",
   "metadata": {
    "id": "ac6f6b42-f351-496b-a7e8-1286426457eb"
   },
   "source": [
    "We could also address it by using a more conventional prompt, describing what we want and how we want the format.\n",
    "\n",
    "However, it's essential to understand that in this case, the model is following instructions, whereas in the case of use shots, it is learning in real-time during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
    "outputId": "4c970dde-37ff-41a9-8d4e-37bb727f47a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Lewis Hamilton\n",
      "Team: Mercedes\n",
      "Points: 413\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\"\"\"You are and expert in f1.\n",
    "    You are going to answer the question of the user giving the name of the rider,\n",
    "    the name of the team and the points of the champion, following the format:\n",
    "    Drive:\n",
    "    Team:\n",
    "    Points: \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "KNDL1GzVngyL",
   "metadata": {
    "id": "KNDL1GzVngyL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Fernando Alonso.\n",
      "Team: Renault.\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are classifying .\n",
    "\n",
    "     Who won the 2010 f1 championship?\n",
    "     Driver: Sebastian Vettel.\n",
    "     Team: Red Bull Renault.\n",
    "\n",
    "     Who won the 2009 f1 championship?\n",
    "     Driver: Jenson Button.\n",
    "     Team: BrawnGP.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qZPNTLMPnkQ4",
   "metadata": {
    "id": "qZPNTLMPnkQ4"
   },
   "source": [
    "Few Shots for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ejcstgTxnnX5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejcstgTxnnX5",
    "outputId": "4b91cc73-18f6-4944-a46b-806b02b7becb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in reviewing product opinions and classifying them as positive or negative.\n",
    "\n",
    "     It fulfilled its function perfectly, I think the price is fair, I would buy it again.\n",
    "     Sentiment: Positive\n",
    "\n",
    "     It didn't work bad, but I wouldn't buy it again, maybe it's a bit expensive for what it does.\n",
    "     Sentiment: Negative.\n",
    "\n",
    "     I wouldn't know what to say, my son uses it, but he doesn't love it.\n",
    "     Sentiment: Neutral\n",
    "     \"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"I'm not going to return it, but I don't plan to buy it again.\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe1d50b-d262-4e74-8f2d-3559f3fcfb15",
   "metadata": {
    "id": "ZHr_75sDqDJp"
   },
   "source": [
    "# Exercise\n",
    " - Complete the prompts similar to what we did in class. \n",
    "     - Try at least 3 versions\n",
    "     - Be creative\n",
    " - Write a one page report summarizing your findings.\n",
    "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
    " - What did you learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a98c49",
   "metadata": {},
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caf856d",
   "metadata": {},
   "source": [
    "Trying IPL Cricket in India as One Version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48de347d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mumbai Indians won the IPL 2015 by defeating Chennai Super Kings in the final.\n"
     ]
    }
   ],
   "source": [
    "# Zero Shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are an expert in Cricket.'}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the IPL 2015?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3f6ce0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Kolkata Knight Riders won the IPL 2012 by defeating the Chennai Super Kings.\n"
     ]
    }
   ],
   "source": [
    "#one-shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in IPL.\n",
    "\n",
    "     Who won the 2011 IPL match?\n",
    "     Team: Chennai Super Kings.\n",
    "     Won by: 58 Runs.\n",
    "     Defeated: Royal Challengers Banglore.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the IPL 2012?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adda59c-ad09-4e9d-88cd-54f42384a5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team: Sunrisers Hyderabad \n",
      "Won by: 8 runs\n",
      "Defeated: Royal Challengers Bangalore\n"
     ]
    }
   ],
   "source": [
    "#Few- shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are and expert in IPL.\\n\\n'},\n",
    "    {'role':'user', 'content':'Who won the 2010 IPL Match?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Team: Chennai Super Kings. \\nWon by: 22 runs. \\nDefeated: Mumbai Indians. \"\"\"},\n",
    "    {'role':'user', 'content':'Who won the 2009 IPL Match?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Team: Deccan Chargers. \\nWon by: 6 runs. \\nDefeated: Royal Challengers Bangalore. \"\"\"},\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\"Who won the IPL 2016?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a55f9d",
   "metadata": {},
   "source": [
    "# Observations:\n",
    "* Zero-shot: Response received is correct but unstructured means it provided additional information.\n",
    "* One-shot: Response received is correct but not accurate as it didnt included \"Won by\" parameter. i understood that one shot prompting may be insufficient for formats.\n",
    "* Few-shot: Response received is correct and accurate as it gave the response in expected format. i understood that few shot prompting will improve the model ability.\n",
    "\n",
    "Issues and Hallucinations: \n",
    "* The one-shot prompt’s failure to include the “Won by” field suggests that GPT-3.5 struggled to generalize the format from a single example. No hallucinations were observed, as all responses were factually correct based on IPL records.\n",
    "\n",
    "Key Finding: \n",
    "* The few-shot approach was more effective for ensuring the desired format, highlighting the need for multiple examples when precise output structure is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae7925",
   "metadata": {},
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2072a6b",
   "metadata": {},
   "source": [
    "largest cities in Germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e417f533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest city in Germany is Berlin.\n"
     ]
    }
   ],
   "source": [
    "# Zero Shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are an expert Geography.'}\n",
    "]\n",
    "print(return_OAIResponse(\"Which is the largest city in Germany?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e49b137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest city in Germany is Berlin.\n"
     ]
    }
   ],
   "source": [
    "#one-shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in Geography.\n",
    "\n",
    "     Which is the largest city in Germany?\n",
    "     City: Berlin.\n",
    "     Land Area: 891.3 km2.\n",
    "     State: Berlin.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Which is the largest city in Germany?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a01885c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City: Munich. \n",
      "Land Area: 310.4 km2. \n",
      "State: Bavaria.\n"
     ]
    }
   ],
   "source": [
    "#Recomended solution\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are an expert in Geography.\\n\\n'},\n",
    "    {'role':'user', 'content':'Which is the first largest city in Germany?'},\n",
    "    {'role':'assistant', 'content':\"\"\"City: Berlin. \\nLand Area: 891.3 km2. \\nState: Berlin. \"\"\"},\n",
    "    {'role':'user', 'content':'Which is the second largest city in Germany?'},\n",
    "    {'role':'assistant', 'content':\"\"\"City: Hamburg. \\nLand Area: 755.2 km2. \\nState: Hamburg. \"\"\"},\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\"Which is the third largest city in Germany?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f769c7",
   "metadata": {},
   "source": [
    "# Observations:\n",
    "* Zero-shot: Response received is correctwitout additional information but unstructured .\n",
    "* One-shot: Response received is correct but not accurate as it didnt included \"Land Area and State\" parameters. i understood that one shot prompting may be insufficient for formats.\n",
    "* Few-shot: Response received is correct and accurate as it gave the response in expected format. i understood that few shot prompting will improve the model ability. i tried giving wrong state in the example for Berlin, yet model gave the expected output\n",
    "\n",
    "Issues and Hallucinations: \n",
    "* The one-shot prompt’s failure to include the “Land Area and State” field suggests that GPT-3.5 struggled to generalize the format from a single example. The model’s ability to provide a correct state for Munich in the few-shot case suggests it relied on its internal knowledge rather than strictly following the flawed. No hallucinations were observed, as all responses were factually correct based on Geographical records.\n",
    "\n",
    "Key Finding: \n",
    "* The few-shot approach was more effective for ensuring the desired format, highlighting the need for multiple examples when precise output structure is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c672d",
   "metadata": {},
   "source": [
    "## Version 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef397012",
   "metadata": {},
   "source": [
    "AI Models by Release Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad0098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The concept of machine learning was introduced in the 1950s with the development of the first artificial neural network by Frank Rosenblatt in 1958, called the Perceptron. This marked the beginning of the formal study and research into computational models that can learn from data and improve over time, laying the foundation for modern machine learning algorithms and techniques.\n"
     ]
    }
   ],
   "source": [
    "# Zero Shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are an expert in artificial intelligence and its history.'}\n",
    "]\n",
    "print(return_OAIResponse(\"When was the Machine learning concept introduced?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fa68d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is a domain of artificial intelligence that involves the development of algorithms and statistical models that enable computers to improve their performance on a specific task through the analysis of data, without being explicitly programmed. In other words, machine learning algorithms use statistical techniques to enable machines to learn from and make predictions or decisions based on data.\n",
      "\n",
      "Key components of machine learning include:\n",
      "\n",
      "1. Data: Machine learning models are trained on large amounts of data, which are used to identify patterns and relationships that can be used to make predictions or classifications.\n",
      "\n",
      "2. Algorithms: Machine learning algorithms are the mathematical representations of the models that are trained on the data. These algorithms can be classified into various categories, such as regression algorithms for predicting continuous values and classification algorithms for identifying categories or classes.\n",
      "\n",
      "3. Models: Machine learning models are the outcomes of the training process, which can be used to make predictions or classifications on new, unseen data. Common types of machine learning models include supervised learning models, unsupervised learning models, and reinforcement learning models.\n",
      "\n",
      "Overall, machine learning has numerous applications in diverse fields, including image and speech recognition, natural language processing, healthcare, finance, and more. It continues to play a crucial role in advancing artificial intelligence technologies and enabling machines to perform increasingly complex tasks.\n"
     ]
    }
   ],
   "source": [
    "#one-shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in artificial intelligence and its history.\n",
    "\n",
    "     Provide details about Machine Learning?\n",
    "     Description: mathematical representations of algorithms trained on data to make predictions or classifications.\n",
    "     Models: Supervised, unsupervised, Reinforcement learning.\n",
    "     Algorithms: Regression, Classification.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Provide details about Machine Learning?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99bf0948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is a branch of artificial intelligence that involves developing algorithms and models that allow computers to learn and make predictions or decisions based on data. Instead of being explicitly programmed, the computer learns from patterns in the data provided to it. There are several types of machine learning approaches including supervised learning, unsupervised learning, and reinforcement learning.\n",
      "\n",
      "In supervised learning, the model is trained on labeled data, with input-output pairs used for learning. The model learns to map inputs to outputs based on the training data. Regression and classification are common tasks in supervised learning.\n",
      "\n",
      "Unsupervised learning, on the other hand, involves training the model on unlabeled data and the model is tasked with finding patterns or structure in the data without explicit guidance. Clustering and dimensionality reduction are common unsupervised learning tasks.\n",
      "\n",
      "Reinforcement learning is a type of machine learning where an agent learns to make decisions by taking actions in an environment and receiving rewards or penalties. The agent learns to maximize its cumulative reward over time by exploring and exploiting different actions.\n",
      "\n",
      "Machine learning has numerous applications across various industries such as healthcare, finance, marketing, and more. Some common applications include image and speech recognition, natural language processing, recommendation systems, and predictive analytics.\n"
     ]
    }
   ],
   "source": [
    "#Recomended solution\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are an expert in artificial intelligence and its history.\\n\\n'},\n",
    "    {'role':'user', 'content':'Can you provide details about Machine Learning?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Description: mathematical representations of algorithms trained on data to make predictions or classifications. \\nModels: Supervised, unsupervised, Reinforcement learning. \\nApplications: Image Recognition, Speech Recognition, Social Media Features, Cybersecurity, Medical Diagnosis, NLP. \"\"\"},\n",
    "    {'role':'user', 'content':'Can you provide details about Deep Learning?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Description: mathematical representations of algorithms trained on data to make predictions or classifications. \\nModels: CNN, RNN, LSTMs, GANs, Transformers, Autoencoders, DBNs, DQNs. \\nApplications: Computer Vision, NLP, Speech Recognition, Robotics, Game Playing, Predictive Analytics. \"\"\"},\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\"Can you provide details about Machine Learning\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5136ab",
   "metadata": {},
   "source": [
    "# Observations:\n",
    "* Zero-shot: The prompt asked about the introduction of the machine learning concept, and the model provided a detailed historical response (1950s, Perceptron by Frank Rosenblatt). While accurate, the response was verbose and not formatted, consistent with zero-shot behavior. .\n",
    "* One-shot: The example provided for this version is structured format but the Response received is long unstructured explanation that included some of the requested components(models, algorithms) but ignored the concise format.i understood that one shot prompting may be insufficient for formats.\n",
    "* Few-shot: Using examples for machine learning and deep learning, the model provided a detailed but unstructured response for machine learning, covering description, models, and applications but not strictly adhering to the example format. The response was accurate but verbose, suggesting that even with multiple examples, the model prioritized its default response style over the specified structure.\n",
    "\n",
    "Issues and Hallucinations: \n",
    "* No hallucinations were observed, as the responses were factually correct. However, the model’s tendency to produce verbose, unstructured text despite examples indicates a limitation in enforcing strict formatting for abstract or broad topics like AI.\n",
    "\n",
    "Key Finding: \n",
    "* Complex or abstract queries may require more explicit instructions or additional examples to achieve the desired format, as the model tends to revert to its natural response style."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4768ca4f",
   "metadata": {},
   "source": [
    "## Learnings\n",
    "* Example Quality Matters: Accurate and clear examples are critical for effective few-shot learning. Errors in examples (e.g., Bremen for Berlin) can confuse the model or be ignored, depending on its internal knowledge.\n",
    "* Few-Shot Superiority: Providing multiple examples significantly improves the model’s ability to adhere to a desired format, especially for structured outputs like sports or geographical data.\n",
    "* Domain Complexity: Simple, fact-based domains (IPL, cities) respond better to few-shot prompting than abstract topics (AI), where the model may prioritize its default response style.\n",
    "* Prompt Design: Using conversation-style prompts with clear user-assistant roles (as in the recommended solutions) enhances learning compared to embedding examples in the system prompt. However, explicit instructions may be needed for complex topics to enforce strict formatting.\n",
    "* Model Limitations: GPT-3.5’s performance in one-shot scenarios suggests that smaller models may require more examples or explicit instructions to achieve consistent formatting, especially for nuanced or complex queries."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
