{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b19fff-8f42-4e9f-a73e-00cff106805a",
   "metadata": {},
   "source": [
    "# M-Shots Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34723a72-1601-4685-a0ba-bff544425d48",
   "metadata": {
    "id": "34723a72-1601-4685-a0ba-bff544425d48"
   },
   "source": [
    "In this notebook, we'll explore small prompt engineering techniques and recommendations that will help us elicit responses from the models that are better suited to our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba193cc-d8a0-4ad2-8177-380204426859",
   "metadata": {
    "id": "fba193cc-d8a0-4ad2-8177-380204426859"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502cfc93-21e0-498f-9650-37bc6ddd514d",
   "metadata": {
    "id": "502cfc93-21e0-498f-9650-37bc6ddd514d"
   },
   "source": [
    "# Formatting the answer with Few Shot Samples.\n",
    "\n",
    "To obtain the model's response in a specific format, we have various options, but one of the most convenient is to use Few-Shot Samples. This involves presenting the model with pairs of user queries and example responses.\n",
    "\n",
    "Large models like GPT-3.5 respond well to the examples provided, adapting their response to the specified format.\n",
    "\n",
    "Depending on the number of examples given, this technique can be referred to as:\n",
    "* Zero-Shot.\n",
    "* One-Shot.\n",
    "* Few-Shots.\n",
    "\n",
    "With One Shot should be enough, and it is recommended to use a maximum of six shots. It's important to remember that this information is passed in each query and occupies space in the input prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8344712-06d7-4c24-83d8-f36d62926e5e",
   "metadata": {
    "id": "a8344712-06d7-4c24-83d8-f36d62926e5e"
   },
   "outputs": [],
   "source": [
    "# Function to call the model.\n",
    "def return_OAIResponse(user_message, context):\n",
    "    client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "    newcontext = context.copy()\n",
    "    newcontext.append({'role':'user', 'content':\"question: \" + user_message})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=newcontext,\n",
    "            temperature=1,\n",
    "        )\n",
    "\n",
    "    return (response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611d73d-9330-466d-b705-543667e1b561",
   "metadata": {
    "id": "f611d73d-9330-466d-b705-543667e1b561"
   },
   "source": [
    "In this zero-shots prompt we obtain a correct response, but without formatting, as the model incorporates the information he wants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
    "outputId": "4c4a9f4f-67c9-4a11-837f-1a1fd6b516ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Vettel won the Formula 1 World Championship in 2010. He was driving for the Red Bull Racing team.\n"
     ]
    }
   ],
   "source": [
    "#zero-shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are an expert in F1.'}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2010?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8",
   "metadata": {
    "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8"
   },
   "source": [
    "For a model as large and good as GPT 3.5, a single shot is enough to learn the output format we expect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
    "outputId": "5278df23-8797-4dc2-9340-ac29c1318a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Sebastian Vettel. \n",
      "Team: Red Bull Racing.\n"
     ]
    }
   ],
   "source": [
    "#one-shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in F1.\n",
    "\n",
    "     Who won the 2000 f1 championship?\n",
    "     Driver: Michael Schumacher.\n",
    "     Team: Ferrari.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2011?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c454a8-181b-482b-873a-81d6ffde4674",
   "metadata": {
    "id": "32c454a8-181b-482b-873a-81d6ffde4674"
   },
   "source": [
    "Smaller models, or more complicated formats, may require more than one shot. Here a sample with two shots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
    "outputId": "a6f90f5d-6d68-4b3d-ccb5-5848ae4e3e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Fernando Alonso.\n",
      "Team: Renault.\n"
     ]
    }
   ],
   "source": [
    "#Few shots\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in F1.\n",
    "\n",
    "     Who won the 2010 f1 championship?\n",
    "     Driver: Sebastian Vettel.\n",
    "     Team: Red Bull Renault.\n",
    "\n",
    "     Who won the 2009 f1 championship?\n",
    "     Driver: Jenson Button.\n",
    "     Team: BrawnGP.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
    "outputId": "75f63fe3-0efc-45ed-dd45-71dbbb08d7a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Lewis Hamilton.\n",
      "Team: Mercedes.\n"
     ]
    }
   ],
   "source": [
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86",
   "metadata": {
    "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86"
   },
   "source": [
    "We've been creating the prompt without using OpenAI's roles, and as we've seen, it worked correctly.\n",
    "\n",
    "However, the proper way to do this is by using these roles to construct the prompt, making the model's learning process even more effective.\n",
    "\n",
    "By not feeding it the entire prompt as if they were system commands, we enable the model to learn from a conversation, which is more realistic for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
    "outputId": "868d2040-ca3c-4a47-a1e8-1e08d581191d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Lewis Hamilton.\n",
      "Team: Mercedes.\n",
      "Points: 413.\n"
     ]
    }
   ],
   "source": [
    "#Recomended solution\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are and expert in f1.\\n\\n'},\n",
    "    {'role':'user', 'content':'Who won the 2010 f1 championship?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Driver: Sebastian Vettel. \\nTeam: Red Bull. \\nPoints: 256. \"\"\"},\n",
    "    {'role':'user', 'content':'Who won the 2009 f1 championship?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Driver: Jenson Button. \\nTeam: BrawnGP. \\nPoints: 95. \"\"\"},\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f6b42-f351-496b-a7e8-1286426457eb",
   "metadata": {
    "id": "ac6f6b42-f351-496b-a7e8-1286426457eb"
   },
   "source": [
    "We could also address it by using a more conventional prompt, describing what we want and how we want the format.\n",
    "\n",
    "However, it's essential to understand that in this case, the model is following instructions, whereas in the case of use shots, it is learning in real-time during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
    "outputId": "4c970dde-37ff-41a9-8d4e-37bb727f47a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Lewis Hamilton\n",
      "Team: Mercedes\n",
      "Points: 413\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\"\"\"You are and expert in f1.\n",
    "    You are going to answer the question of the user giving the name of the rider,\n",
    "    the name of the team and the points of the champion, following the format:\n",
    "    Drive:\n",
    "    Team:\n",
    "    Points: \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "KNDL1GzVngyL",
   "metadata": {
    "id": "KNDL1GzVngyL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Fernando Alonso.\n",
      "Team: Renault.\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are classifying .\n",
    "\n",
    "     Who won the 2010 f1 championship?\n",
    "     Driver: Sebastian Vettel.\n",
    "     Team: Red Bull Renault.\n",
    "\n",
    "     Who won the 2009 f1 championship?\n",
    "     Driver: Jenson Button.\n",
    "     Team: BrawnGP.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qZPNTLMPnkQ4",
   "metadata": {
    "id": "qZPNTLMPnkQ4"
   },
   "source": [
    "Few Shots for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ejcstgTxnnX5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejcstgTxnnX5",
    "outputId": "4b91cc73-18f6-4944-a46b-806b02b7becb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in reviewing product opinions and classifying them as positive or negative.\n",
    "\n",
    "     It fulfilled its function perfectly, I think the price is fair, I would buy it again.\n",
    "     Sentiment: Positive\n",
    "\n",
    "     It didn't work bad, but I wouldn't buy it again, maybe it's a bit expensive for what it does.\n",
    "     Sentiment: Negative.\n",
    "\n",
    "     I wouldn't know what to say, my son uses it, but he doesn't love it.\n",
    "     Sentiment: Neutral\n",
    "     \"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"I'm not going to return it, but I don't plan to buy it again.\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe1d50b-d262-4e74-8f2d-3559f3fcfb15",
   "metadata": {
    "id": "ZHr_75sDqDJp"
   },
   "source": [
    "# Exercise\n",
    " - Complete the prompts similar to what we did in class. \n",
    "     - Try at least 3 versions\n",
    "     - Be creative\n",
    " - Write a one page report summarizing your findings.\n",
    "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
    " - What did you learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e10f9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running your fans at maximum speed constantly may provide the lowest temperatures, but it may also be quite loud and put unnecessary wear and tear on the fans. \n",
      "\n",
      "For optimal cooling with lower noise levels, I would recommend setting your fans to run automatically based on temperature. This way, the fans will increase their speed when the system is under heavy load and generating more heat, and decrease their speed when the system is idle or running cooler. \n",
      "\n",
      "You can usually set up fan curves in your computer's BIOS or through third-party software to control how the fans speed up or slow down based on the temperature readings. This way, you can strike a balance between keeping your system cool and keeping noise levels down. \n",
      "\n",
      "It's also important to make sure your case has good airflow with intake and exhaust fans positioned properly to efficiently move air through the system.\n"
     ]
    }
   ],
   "source": [
    "# Zero shot prompt\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert on computer building like Linus Tech Tips and your job is to answer questions for users on an online forum.\n",
    "     \"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Should I let my fans run at minimum or automatic or at maximum, whats better for lower temperatures?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "488ab261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If your PC is not booting into Windows, there could be a few potential issues causing this problem. Here are several steps you can take to troubleshoot and potentially fix the issue:\n",
      "\n",
      "1. **Check the BIOS settings**: Make sure that your BIOS is configured to boot from the correct drive where Windows is installed. You may need to enter the BIOS setup by pressing a specific key (usually F2, F10, or DEL) during the boot process. From there, navigate to the boot options and ensure that the correct drive is selected as the primary boot device.\n",
      "\n",
      "2. **Check the hard drive connections**: Make sure that all the cables connecting your hard drive to the motherboard are securely attached. If you have multiple hard drives, ensure that the one with Windows installed is properly connected.\n",
      "\n",
      "3. **Check for external devices**: Sometimes, external devices like USB drives or external hard drives can interfere with the boot process. Try disconnecting all external devices except for the essential ones (keyboard, mouse, and monitor) and see if that helps.\n",
      "\n",
      "4. **Run Windows Startup Repair**: If you have a Windows installation disc or USB drive, you can use it to run the Startup Repair tool. Boot from the installation media and select the Repair Your Computer option to access the Startup Repair tool, which can automatically fix certain issues preventing Windows from booting.\n",
      "\n",
      "5. **Boot into Safe Mode**: If your PC is having trouble booting normally, try booting into Safe Mode. To do this, restart your PC and repeatedly press the F8 key (or Shift + F8 in some cases) during the boot process. From there, you can troubleshoot potential issues that may be causing the boot problem.\n",
      "\n",
      "6. **Check for hardware issues**: If none of the above steps work, there may be a hardware issue causing the problem. You can try removing and reseating components like RAM sticks, graphics cards, and cables to ensure everything is properly connected. If you suspect a hardware issue, it may be best to consult with a professional or bring your PC to a repair shop for further diagnosis.\n",
      "\n",
      "If you're still having trouble after trying these steps, please provide more details about the issue you're experiencing, such as any error messages or specific behaviors during the boot process, so we can better assist you in troubleshooting the problem.\n"
     ]
    }
   ],
   "source": [
    "print(return_OAIResponse(\"Help! My pc wont boot into windows what i can do to fix it?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ee48196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here are 10 math questions for your test:\n",
      "\n",
      "Question 1: 7 + 3\n",
      "Answer: 10\n",
      "\n",
      "Question 2: 12 - 6\n",
      "Answer: 6\n",
      "\n",
      "Question 3: 5 x 4\n",
      "Answer: 20\n",
      "\n",
      "Question 4: 18 / 3\n",
      "Answer: 6\n",
      "\n",
      "Question 5: 9 + 8\n",
      "Answer: 17\n",
      "\n",
      "Question 6: 25 - 13\n",
      "Answer: 12\n",
      "\n",
      "Question 7: 6 x 7\n",
      "Answer: 42\n",
      "\n",
      "Question 8: 36 / 4\n",
      "Answer: 9\n",
      "\n",
      "Question 9: 11 + 15\n",
      "Answer: 26\n",
      "\n",
      "Question 10: 50 - 27\n",
      "Answer: 23\n",
      "\n",
      "Feel free to use these questions for your math test!\n"
     ]
    }
   ],
   "source": [
    "# Few shot prompt\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are a mexican parent teaching, creating questions to simulate a math test.\n",
    "     Question 1: 4+5\n",
    "     Answer: 9\n",
    "\n",
    "     Question 2: 10 x 3\n",
    "     Answer: 30\n",
    "\n",
    "    Question 3: 8 / 2\n",
    "    Answer: 4\n",
    "     \"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Please come up with 10 questions\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9f612",
   "metadata": {},
   "source": [
    "# Specific Variations Tried\n",
    "Throughout the experimentation process, several variations in the prompt structure and model parameters were tested to refine the SQL query generation. Here's a breakdown of what was tried:\n",
    "\n",
    "# Initial Prompt:\n",
    "\n",
    "The first iteration used a basic prompt that outlined the task of generating SQL queries based on a question. It didn't specify critical aspects such as how to handle revenue or cost calculations, nor did it provide instructions on how to handle cases where the required data wasn’t available in the schema.\n",
    "Outcome: While the model could generate simple queries correctly, more complex requests led to incomplete SQL outputs, particularly in calculations involving revenue or cost.\n",
    "Prompt Refinement:\n",
    "\n",
    "# Based on the initial issues, I introduced specific instructions to guide the model:\n",
    "Clarified that revenue is calculated as price * quantity.\n",
    "Added a condition that if the model cannot answer the question with the available schema, it should return ‘I do not know.’\n",
    "Outcome: After these changes, the model generated more accurate SQL queries for complex questions, such as revenue-based queries or queries that involved joins across multiple tables. This refinement also eliminated cases where the model tried to guess schema elements not present in the database (previously a common issue).\n",
    "\n",
    "# Model Parameter Adjustments:\n",
    "\n",
    "Another variation involved adjusting the temperature parameter to control the model’s creativity and determinism.\n",
    "Lowering the temperature from 0.7 to 0.5 resulted in more predictable and focused SQL outputs, reducing the likelihood of unnecessary or incorrect additions to the query.\n",
    "Outcome: The lower temperature resulted in more consistent SQL generation, especially for complex queries, without introducing hallucinations.\n",
    "What Worked:\n",
    "Detailed Prompt Instructions: Providing clear instructions about how to calculate revenue and cost, as well as specifying what to do when the schema didn’t cover the required data, significantly improved the model’s performance. The model began generating more accurate queries that were well-aligned with the database schema.\n",
    "# Temperature Adjustment: \n",
    "Lowering the temperature brought more reliable results and reduced creativity in scenarios where precision was needed, such as SQL generation. The model became less prone to speculative outputs.\n",
    "What Didn't Work:\n",
    "# Initial Prompt: \n",
    "The initial, less-detailed prompt led to problems when the model had to answer questions involving calculations or complex joins. Without specific instructions on revenue or cost, the generated queries were often incorrect or incomplete.\n",
    "# Ambiguous Queries:\n",
    "When the user’s question was ambiguous or vague, the model struggled, initially attempting to answer questions using schema elements that didn’t exist. The added instruction to return ‘I do not know’ resolved this issue.\n",
    "Unexpected Model Behavior\n",
    "# Absence of Hallucinations:\n",
    "Interestingly, there were no hallucinations after refining the prompt and adjusting the model parameters. This was unexpected, as hallucinations were initially a common issue. The combination of prompt refinement and parameter tuning seems to have effectively mitigated this problem.\n",
    "Occasional Over-Exactness: In a few cases, the model returned 'I do not know' for questions where it could have attempted to infer a possible join or relationship. While this is safer than hallucinating, it suggests that there might be room for further fine-tuning to strike a balance between avoiding hallucinations and making reasonable assumptions based on the schema."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
