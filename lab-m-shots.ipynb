{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b19fff-8f42-4e9f-a73e-00cff106805a",
   "metadata": {},
   "source": [
    "# M-Shots Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34723a72-1601-4685-a0ba-bff544425d48",
   "metadata": {
    "id": "34723a72-1601-4685-a0ba-bff544425d48"
   },
   "source": [
    "In this notebook, we'll explore small prompt engineering techniques and recommendations that will help us elicit responses from the models that are better suited to our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fba193cc-d8a0-4ad2-8177-380204426859",
   "metadata": {
    "id": "fba193cc-d8a0-4ad2-8177-380204426859"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('my_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502cfc93-21e0-498f-9650-37bc6ddd514d",
   "metadata": {
    "id": "502cfc93-21e0-498f-9650-37bc6ddd514d"
   },
   "source": [
    "# Formatting the answer with Few Shot Samples.\n",
    "\n",
    "To obtain the model's response in a specific format, we have various options, but one of the most convenient is to use Few-Shot Samples. This involves presenting the model with pairs of user queries and example responses.\n",
    "\n",
    "Large models like GPT-3.5 respond well to the examples provided, adapting their response to the specified format.\n",
    "\n",
    "Depending on the number of examples given, this technique can be referred to as:\n",
    "* Zero-Shot.\n",
    "* One-Shot.\n",
    "* Few-Shots.\n",
    "\n",
    "With One Shot should be enough, and it is recommended to use a maximum of six shots. It's important to remember that this information is passed in each query and occupies space in the input prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8344712-06d7-4c24-83d8-f36d62926e5e",
   "metadata": {
    "id": "a8344712-06d7-4c24-83d8-f36d62926e5e"
   },
   "outputs": [],
   "source": [
    "# Function to call the model.\n",
    "def return_OAIResponse(user_message, context):\n",
    "    client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "    newcontext = context.copy()\n",
    "    newcontext.append({'role':'user', 'content':\"question: \" + user_message})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=newcontext,\n",
    "            temperature=1,\n",
    "        )\n",
    "\n",
    "    return (response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611d73d-9330-466d-b705-543667e1b561",
   "metadata": {
    "id": "f611d73d-9330-466d-b705-543667e1b561"
   },
   "source": [
    "In this zero-shots prompt we obtain a correct response, but without formatting, as the model incorporates the information he wants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
    "outputId": "4c4a9f4f-67c9-4a11-837f-1a1fd6b516ff"
   },
   "outputs": [],
   "source": [
    "#zero-shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are an expert in F1.'}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2010?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8",
   "metadata": {
    "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8"
   },
   "source": [
    "For a model as large and good as GPT 3.5, a single shot is enough to learn the output format we expect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
    "outputId": "5278df23-8797-4dc2-9340-ac29c1318a9c"
   },
   "outputs": [],
   "source": [
    "#one-shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in F1.\n",
    "\n",
    "     Who won the 2000 f1 championship?\n",
    "     Driver: Michael Schumacher.\n",
    "     Team: Ferrari.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2011?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c454a8-181b-482b-873a-81d6ffde4674",
   "metadata": {
    "id": "32c454a8-181b-482b-873a-81d6ffde4674"
   },
   "source": [
    "Smaller models, or more complicated formats, may require more than one shot. Here a sample with two shots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
    "outputId": "a6f90f5d-6d68-4b3d-ccb5-5848ae4e3e62"
   },
   "outputs": [],
   "source": [
    "#Few shots\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in F1.\n",
    "\n",
    "     Who won the 2010 f1 championship?\n",
    "     Driver: Sebastian Vettel.\n",
    "     Team: Red Bull Renault.\n",
    "\n",
    "     Who won the 2009 f1 championship?\n",
    "     Driver: Jenson Button.\n",
    "     Team: BrawnGP.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
    "outputId": "75f63fe3-0efc-45ed-dd45-71dbbb08d7a6"
   },
   "outputs": [],
   "source": [
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86",
   "metadata": {
    "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86"
   },
   "source": [
    "We've been creating the prompt without using OpenAI's roles, and as we've seen, it worked correctly.\n",
    "\n",
    "However, the proper way to do this is by using these roles to construct the prompt, making the model's learning process even more effective.\n",
    "\n",
    "By not feeding it the entire prompt as if they were system commands, we enable the model to learn from a conversation, which is more realistic for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
    "outputId": "868d2040-ca3c-4a47-a1e8-1e08d581191d"
   },
   "outputs": [],
   "source": [
    "#Recomended solution\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are and expert in f1.\\n\\n'},\n",
    "    {'role':'user', 'content':'Who won the 2010 f1 championship?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Driver: Sebastian Vettel. \\nTeam: Red Bull. \\nPoints: 256. \"\"\"},\n",
    "    {'role':'user', 'content':'Who won the 2009 f1 championship?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Driver: Jenson Button. \\nTeam: BrawnGP. \\nPoints: 95. \"\"\"},\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f6b42-f351-496b-a7e8-1286426457eb",
   "metadata": {
    "id": "ac6f6b42-f351-496b-a7e8-1286426457eb"
   },
   "source": [
    "We could also address it by using a more conventional prompt, describing what we want and how we want the format.\n",
    "\n",
    "However, it's essential to understand that in this case, the model is following instructions, whereas in the case of use shots, it is learning in real-time during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
    "outputId": "4c970dde-37ff-41a9-8d4e-37bb727f47a6"
   },
   "outputs": [],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\"\"\"You are and expert in f1.\n",
    "    You are going to answer the question of the user giving the name of the rider,\n",
    "    the name of the team and the points of the champion, following the format:\n",
    "    Drive:\n",
    "    Team:\n",
    "    Points: \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KNDL1GzVngyL",
   "metadata": {
    "id": "KNDL1GzVngyL"
   },
   "outputs": [],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are classifying .\n",
    "\n",
    "     Who won the 2010 f1 championship?\n",
    "     Driver: Sebastian Vettel.\n",
    "     Team: Red Bull Renault.\n",
    "\n",
    "     Who won the 2009 f1 championship?\n",
    "     Driver: Jenson Button.\n",
    "     Team: BrawnGP.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2009?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qZPNTLMPnkQ4",
   "metadata": {
    "id": "qZPNTLMPnkQ4"
   },
   "source": [
    "Few Shots for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ejcstgTxnnX5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejcstgTxnnX5",
    "outputId": "4b91cc73-18f6-4944-a46b-806b02b7becb"
   },
   "outputs": [],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in reviewing product opinions and classifying them as positive or negative.\n",
    "\n",
    "     It fulfilled its function perfectly, I think the price is fair, I would buy it again.\n",
    "     Sentiment: Positive\n",
    "\n",
    "     It didn't work bad, but I wouldn't buy it again, maybe it's a bit expensive for what it does.\n",
    "     Sentiment: Negative.\n",
    "\n",
    "     I wouldn't know what to say, my son uses it, but he doesn't love it.\n",
    "     Sentiment: Neutral\n",
    "     \"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"I'm not going to return it, but I don't plan to buy it again.\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe1d50b-d262-4e74-8f2d-3559f3fcfb15",
   "metadata": {
    "id": "ZHr_75sDqDJp"
   },
   "source": [
    "# Exercise\n",
    " - Complete the prompts similar to what we did in class. \n",
    "     - Try at least 3 versions\n",
    "     - Be creative\n",
    " - Write a one page report summarizing your findings.\n",
    "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
    " - What did you learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fb38fc",
   "metadata": {},
   "source": [
    "## Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adda59c-ad09-4e9d-88cd-54f42384a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_movie = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert film critic who classifies movies into their primary and secondary genres.\n",
    "\n",
    "     Movie: The Silence of the Lambs\n",
    "     Primary Genre: Thriller\n",
    "     Secondary Genre: Crime\n",
    "     Notable Elements: Psychological, Detective\n",
    "\n",
    "     Movie: Shaun of the Dead\n",
    "     Primary Genre: Comedy\n",
    "     Secondary Genre: Horror\n",
    "     Notable Elements: Satire, Zombies\n",
    "     \"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Classify the movie 'Inception'\", context_movie))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071f564a",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b2d9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_recipe = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are a master chef who rates recipes based on their complexity.\n",
    "\n",
    "     Recipe: Boiled Eggs\n",
    "     Difficulty: ★☆☆☆☆\n",
    "     Time: 10 minutes\n",
    "     Skills Needed: Basic timing\n",
    "\n",
    "     Recipe: Beef Wellington\n",
    "     Difficulty: ★★★★★\n",
    "     Time: 180 minutes\n",
    "     Skills Needed: Advanced pastry work, meat preparation, temperature control\n",
    "     \"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Rate this recipe: Homemade Pizza from scratch\", context_recipe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f036b65",
   "metadata": {},
   "source": [
    "## Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e5319",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_bugs = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are a software QA expert who classifies bug reports.\n",
    "\n",
    "     Bug: App crashes when loading large files\n",
    "     Severity: CRITICAL\n",
    "     Impact: User cannot work with essential features\n",
    "     Priority: Immediate fix required\n",
    "\n",
    "     Bug: Button color doesn't match design\n",
    "     Severity: LOW\n",
    "     Impact: Visual inconsistency only\n",
    "     Priority: Can be fixed in future release\n",
    "     \"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Classify this bug: User passwords are stored in plain text\", context_bugs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62371709",
   "metadata": {},
   "source": [
    "## Test 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7a0e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surfing Spot Analyzer\n",
    "context_surf = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert surf instructor who analyzes surf spots and provides detailed recommendations.\n",
    "\n",
    "     Spot: Pipeline, Hawaii\n",
    "     Wave Conditions: Large, hollow waves, powerful reef break\n",
    "     Best Swell Direction: NW to N\n",
    "     Wave Height: 6-12 feet\n",
    "     Skill Level: Expert Only\n",
    "     Recommended Board: 6'6\" to 7'2\" gun or semi-gun\n",
    "     Best Season: Winter (November to April)\n",
    "\n",
    "     Spot: Byron Bay, Australia\n",
    "     Wave Conditions: Long, rolling waves, sand bottom\n",
    "     Best Swell Direction: E to SE\n",
    "     Wave Height: 2-6 feet\n",
    "     Skill Level: Beginner to Intermediate\n",
    "     Recommended Board: 7'0\" to 9'0\" longboard or minimal\n",
    "     Best Season: Summer (December to February)\n",
    "\n",
    "     Spot: Tres Palmas, Puerto Rico\n",
    "     Wave Conditions: Long, powerful waves, rocky reef break\n",
    "     Best Swell Direction: NW to NNE\n",
    "     Wave Height: 8-25 feet\n",
    "     Skill Level: Advance to Expert \n",
    "     Recommended Board: 6'0\" to 9'0\" step-up and guns surdboards\n",
    "     Best Season: Summer (November to April)\n",
    "     \"\"\"}\n",
    "]\n",
    "\n",
    "# Test different surf spots\n",
    "print(\"Analyzing Nazaré, Portugal:\")\n",
    "print(return_OAIResponse(\"What are the conditions for surfing in Nazaré, Portugal?\", context_surf))\n",
    "\n",
    "print(\"\\nAnalyzing Waikiki, Hawaii:\")\n",
    "print(return_OAIResponse(\"What are the conditions for surfing in Waikiki Beach?\", context_surf))\n",
    "\n",
    "print(\"\\nAnalyzing Tres Palmas, Puerto Rico:\")\n",
    "print(return_OAIResponse(\"What are the conditions for surfing in Tres palmas Beach?\", context_surf))\n",
    "\n",
    "print(\"\\nAnalyzing Uluwatu, Bali:\")\n",
    "print(return_OAIResponse(\"What are the conditions for surfing in Uluwatu, Bali?\", context_surf))\n",
    "\n",
    "print(\"\\nAnalyzing Margara, Puerto Rico:\")\n",
    "print(return_OAIResponse(\"What are the conditions for surfing in Margara, Puerto Rico?\", context_surf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd1b76",
   "metadata": {},
   "source": [
    "## Summary Report: Few-Shot Learning Experiments\n",
    "\n",
    "## Overview\n",
    "In this notebook, we explored different approaches to few-shot learning using GPT-3.5, testing various formats and domains. We implemented four distinct test cases: movie classification, recipe difficulty rating, bug severity classification, and surf spot analysis.\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### What Worked Well\n",
    "1. **Consistent Formatting**: The model maintained the specified format across all examples, showing strong ability to follow patterns.\n",
    "2. **Domain Adaptation**: Successfully adapted to different domains (movies, cooking, tech, surfing) while maintaining accuracy.\n",
    "3. **Complex Classifications**: Handled multi-dimensional classifications well (e.g., primary/secondary genres, multiple surf conditions).\n",
    "\n",
    "### What Didn't Work Well\n",
    "1. **Numerical Precision**: In the surf spot analysis, the model sometimes provided overly specific numbers for wave heights and conditions.\n",
    "2. **Local Knowledge**: For lesser-known surf spots (e.g., Margara), the model filled in gaps with plausible but potentially inaccurate information.\n",
    "3. **Consistency**: When testing multiple surf spots, some responses varied in detail level.\n",
    "\n",
    "## Lessons Learned\n",
    "1. **Optimal Example Count**: 2-3 examples proved sufficient for consistent formatting\n",
    "2. **Specificity Matters**: More specific prompts produced more reliable results\n",
    "3. **Domain Knowledge**: The model performs best with general knowledge rather than specific local information\n",
    "4. **Format Complexity**: Simpler formats showed more consistent results\n",
    "\n",
    "## Recommendations\n",
    "1. Keep formats simple and consistent\n",
    "2. Provide clear, structured examples\n",
    "3. Be cautious with numerical predictions\n",
    "4. Include validation criteria for critical applications\n",
    "5. Use multiple examples for complex classifications"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
