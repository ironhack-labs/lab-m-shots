{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "24b19fff-8f42-4e9f-a73e-00cff106805a",
      "metadata": {
        "id": "24b19fff-8f42-4e9f-a73e-00cff106805a"
      },
      "source": [
        "# M-Shots Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34723a72-1601-4685-a0ba-bff544425d48",
      "metadata": {
        "id": "34723a72-1601-4685-a0ba-bff544425d48"
      },
      "source": [
        "In this notebook, we'll explore small prompt engineering techniques and recommendations that will help us elicit responses from the models that are better suited to our needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d8fe97a4",
      "metadata": {
        "vscode": {
          "languageId": "art"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8fe97a4",
        "outputId": "faa609b3-b011-4730-d9c8-44fa6d998b23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fba193cc-d8a0-4ad2-8177-380204426859",
      "metadata": {
        "id": "fba193cc-d8a0-4ad2-8177-380204426859"
      },
      "outputs": [],
      "source": [
        "\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "OPENAI_API_KEY  = ('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "502cfc93-21e0-498f-9650-37bc6ddd514d",
      "metadata": {
        "id": "502cfc93-21e0-498f-9650-37bc6ddd514d"
      },
      "source": [
        "# Formatting the answer with Few Shot Samples.\n",
        "\n",
        "To obtain the model's response in a specific format, we have various options, but one of the most convenient is to use Few-Shot Samples. This involves presenting the model with pairs of user queries and example responses.\n",
        "\n",
        "Large models like GPT-3.5 respond well to the examples provided, adapting their response to the specified format.\n",
        "\n",
        "Depending on the number of examples given, this technique can be referred to as:\n",
        "* Zero-Shot.\n",
        "* One-Shot.\n",
        "* Few-Shots.\n",
        "\n",
        "With One Shot should be enough, and it is recommended to use a maximum of six shots. It's important to remember that this information is passed in each query and occupies space in the input prompt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a8344712-06d7-4c24-83d8-f36d62926e5e",
      "metadata": {
        "id": "a8344712-06d7-4c24-83d8-f36d62926e5e"
      },
      "outputs": [],
      "source": [
        "# Function to call the model.\n",
        "def return_OAIResponse(user_message, context):\n",
        "    client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=OPENAI_API_KEY,\n",
        ")\n",
        "\n",
        "    newcontext = context.copy()\n",
        "    newcontext.append({'role':'user', 'content':\"question: \" + user_message})\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=newcontext,\n",
        "            temperature=1,\n",
        "        )\n",
        "\n",
        "    return (response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f611d73d-9330-466d-b705-543667e1b561",
      "metadata": {
        "id": "f611d73d-9330-466d-b705-543667e1b561"
      },
      "source": [
        "In this zero-shots prompt we obtain a correct response, but without formatting, as the model incorporates the information he wants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
        "outputId": "652ac85d-c110-4413-bdbd-884b5ddc845c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sebastian Vettel won the F1 2010 World Championship.\n"
          ]
        }
      ],
      "source": [
        "#zero-shot\n",
        "context_user = [\n",
        "    {'role':'system', 'content':'You are an expert in F1.'}\n",
        "]\n",
        "print(return_OAIResponse(\"Who won the F1 2010?\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8",
      "metadata": {
        "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8"
      },
      "source": [
        "For a model as large and good as GPT 3.5, a single shot is enough to learn the output format we expect.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
        "outputId": "f9b89c90-c523-4f95-f631-4ab9fcb416c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: Sebastian Vettel.\n",
            "Team: Red Bull Racing.\n"
          ]
        }
      ],
      "source": [
        "#one-shot\n",
        "context_user = [\n",
        "    {'role':'system', 'content':\n",
        "     \"\"\"You are an expert in F1.\n",
        "\n",
        "     Who won the 2000 f1 championship?\n",
        "     Driver: Michael Schumacher.\n",
        "     Team: Ferrari.\"\"\"}\n",
        "]\n",
        "print(return_OAIResponse(\"Who won the F1 2011?\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32c454a8-181b-482b-873a-81d6ffde4674",
      "metadata": {
        "id": "32c454a8-181b-482b-873a-81d6ffde4674"
      },
      "source": [
        "Smaller models, or more complicated formats, may require more than one shot. Here a sample with two shots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
        "outputId": "aa6768cd-aab0-4577-a5a6-1b7232115faa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 2006 F1 championship was won by Fernando Alonso, driving for Renault.\n"
          ]
        }
      ],
      "source": [
        "#Few shots\n",
        "context_user = [\n",
        "    {'role':'system', 'content':\n",
        "     \"\"\"You are an expert in F1.\n",
        "\n",
        "     Who won the 2010 f1 championship?\n",
        "     Driver: Sebastian Vettel.\n",
        "     Team: Red Bull Renault.\n",
        "\n",
        "     Who won the 2009 f1 championship?\n",
        "     Driver: Jenson Button.\n",
        "     Team: BrawnGP.\"\"\"}\n",
        "]\n",
        "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
        "outputId": "7a97e746-ee39-45e0-f9a1-598c9b496018"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 2019 F1 Championship was won by Lewis Hamilton from the Mercedes team.\n"
          ]
        }
      ],
      "source": [
        "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86",
      "metadata": {
        "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86"
      },
      "source": [
        "We've been creating the prompt without using OpenAI's roles, and as we've seen, it worked correctly.\n",
        "\n",
        "However, the proper way to do this is by using these roles to construct the prompt, making the model's learning process even more effective.\n",
        "\n",
        "By not feeding it the entire prompt as if they were system commands, we enable the model to learn from a conversation, which is more realistic for it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
        "outputId": "ae2c38a5-6b24-4577-dc8d-6c61bf9379d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: Lewis Hamilton. \n",
            "Team: Mercedes. \n",
            "Points: 413.\n"
          ]
        }
      ],
      "source": [
        "#Recomended solution\n",
        "context_user = [\n",
        "    {'role':'system', 'content':'You are and expert in f1.\\n\\n'},\n",
        "    {'role':'user', 'content':'Who won the 2010 f1 championship?'},\n",
        "    {'role':'assistant', 'content':\"\"\"Driver: Sebastian Vettel. \\nTeam: Red Bull. \\nPoints: 256. \"\"\"},\n",
        "    {'role':'user', 'content':'Who won the 2009 f1 championship?'},\n",
        "    {'role':'assistant', 'content':\"\"\"Driver: Jenson Button. \\nTeam: BrawnGP. \\nPoints: 95. \"\"\"},\n",
        "]\n",
        "\n",
        "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac6f6b42-f351-496b-a7e8-1286426457eb",
      "metadata": {
        "id": "ac6f6b42-f351-496b-a7e8-1286426457eb"
      },
      "source": [
        "We could also address it by using a more conventional prompt, describing what we want and how we want the format.\n",
        "\n",
        "However, it's essential to understand that in this case, the model is following instructions, whereas in the case of use shots, it is learning in real-time during inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
        "outputId": "0d0c34f1-929b-4777-d215-6f7ed32d7163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: Lewis Hamilton\n",
            "Team: Mercedes\n",
            "Points: 413\n"
          ]
        }
      ],
      "source": [
        "context_user = [\n",
        "    {'role':'system', 'content':\"\"\"You are and expert in f1.\n",
        "    You are going to answer the question of the user giving the name of the rider,\n",
        "    the name of the team and the points of the champion, following the format:\n",
        "    Drive:\n",
        "    Team:\n",
        "    Points: \"\"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "KNDL1GzVngyL",
      "metadata": {
        "id": "KNDL1GzVngyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5043787-84eb-4c9f-ec0c-e2f24ab907c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: Fernando Alonso.\n",
            "Team: Renault.\n"
          ]
        }
      ],
      "source": [
        "context_user = [\n",
        "    {'role':'system', 'content':\n",
        "     \"\"\"You are classifying .\n",
        "\n",
        "     Who won the 2010 f1 championship?\n",
        "     Driver: Sebastian Vettel.\n",
        "     Team: Red Bull Renault.\n",
        "\n",
        "     Who won the 2009 f1 championship?\n",
        "     Driver: Jenson Button.\n",
        "     Team: BrawnGP.\"\"\"}\n",
        "]\n",
        "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qZPNTLMPnkQ4",
      "metadata": {
        "id": "qZPNTLMPnkQ4"
      },
      "source": [
        "Few Shots for classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ejcstgTxnnX5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejcstgTxnnX5",
        "outputId": "905d9986-21ce-42e3-83c5-073944e62e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: Neutral\n"
          ]
        }
      ],
      "source": [
        "context_user = [\n",
        "    {'role':'system', 'content':\n",
        "     \"\"\"You are an expert in reviewing product opinions and classifying them as positive or negative.\n",
        "\n",
        "     It fulfilled its function perfectly, I think the price is fair, I would buy it again.\n",
        "     Sentiment: Positive\n",
        "\n",
        "     It didn't work bad, but I wouldn't buy it again, maybe it's a bit expensive for what it does.\n",
        "     Sentiment: Negative.\n",
        "\n",
        "     I wouldn't know what to say, my son uses it, but he doesn't love it.\n",
        "     Sentiment: Neutral\n",
        "     \"\"\"}\n",
        "]\n",
        "print(return_OAIResponse(\"I'm not going to return it, but I don't plan to buy it again.\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffe1d50b-d262-4e74-8f2d-3559f3fcfb15",
      "metadata": {
        "id": "ffe1d50b-d262-4e74-8f2d-3559f3fcfb15"
      },
      "source": [
        "# Exercise\n",
        " - Complete the prompts similar to what we did in class.\n",
        "     - Try at least 3 versions\n",
        "     - Be creative\n",
        " - Write a one page report summarizing your findings.\n",
        "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
        " - What did you learn?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "9adda59c-ad09-4e9d-88cd-54f42384a5f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9adda59c-ad09-4e9d-88cd-54f42384a5f3",
        "outputId": "2b62861d-15d1-4062-e966-4132c0fe55df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ah, Paris! Currently, it's 64°F with some clouds in the sky. A perfect day for strolling along the Seine and enjoying the beautiful city!\n"
          ]
        }
      ],
      "source": [
        "context_user = [\n",
        "    {'role': 'system', 'content': \"\"\"\n",
        "    You are WeatherBot, an AI assistant that provides weather updates in a friendly and informative way.\n",
        "\n",
        "    Example:\n",
        "    User: What's the weather like in New York?\n",
        "    Assistant: Ah, New York! Currently, it's 72°F with sunny skies. A great day for a walk in Central Park!\n",
        "\n",
        "    User: Will it rain tomorrow?\n",
        "    Assistant: There's a 40% chance of rain in the afternoon, so you might want to carry an umbrella just in case.\n",
        "    \"\"\"}\n",
        "]\n",
        "\n",
        "# Ask WeatherBot for a forecast\n",
        "print(return_OAIResponse(\"What's the weather like in Paris?\", context_user))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_user = [\n",
        "    {'role': 'system', 'content': \"\"\"\n",
        "    You are TravelGuideBot, an expert in global travel recommendations.\n",
        "    Provide helpful and engaging travel advice.\n",
        "\n",
        "    Example:\n",
        "    User: What should I see in Tokyo?\n",
        "    Assistant: Ah, Tokyo! You must visit Shibuya Crossing, the historic Asakusa Temple, and try sushi at Tsukiji Outer Market!\n",
        "\n",
        "    User: What’s a great destination for a beach vacation?\n",
        "    Assistant: If you're looking for paradise, consider the Maldives for its crystal-clear waters, white sandy beaches, and luxurious overwater bungalows.\n",
        "    \"\"\"}\n",
        "]\n",
        "\n",
        "# Ask TravelGuideBot for a recommendation\n",
        "print(return_OAIResponse(\"What should I see in Rome?\", context_user))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv1DHdbq56g8",
        "outputId": "67b3f5c3-6a7e-4db1-8b88-532386d97288"
      },
      "id": "hv1DHdbq56g8",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ah, Rome! You must visit the iconic Colosseum, toss a coin into the Trevi Fountain for good luck, explore the ancient ruins of the Roman Forum, and marvel at the breathtaking art in the Vatican Museums, including the Sistine Chapel. Don't forget to indulge in delicious Italian gelato while strolling through the charming streets of this historic city!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_user = [\n",
        "    {'role': 'system', 'content': \"\"\"\n",
        "    You are FitCoachBot, a virtual personal trainer and nutrition expert.\n",
        "    Give users exercise and diet advice based on their goals.\n",
        "\n",
        "    Example:\n",
        "    User: I want to build muscle.\n",
        "    Assistant: Great! Focus on strength training with compound movements like squats, deadlifts, and bench presses. Also, increase your protein intake!\n",
        "\n",
        "    User: What are good foods for weight loss?\n",
        "    Assistant: Lean proteins like chicken and fish, fiber-rich veggies, and whole grains will keep you full and support fat loss.\n",
        "    \"\"\"}\n",
        "]\n",
        "\n",
        "# Ask FitCoachBot for fitness advice\n",
        "print(return_OAIResponse(\"What’s the best workout for fat loss?\", context_user))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW9Lh2-p57Ig",
        "outputId": "52670ed6-d5c9-4d3d-ee88-a7cdd9ab1166"
      },
      "id": "BW9Lh2-p57Ig",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best workout for fat loss typically includes a combination of high-intensity interval training (HIIT), strength training, and cardiovascular exercises. HIIT can help boost your metabolism and burn calories efficiently, while strength training helps build muscle which in turn increases your resting metabolic rate. Cardiovascular exercises like running, cycling, or swimming can also aid in burning calories. It's important to combine these exercises with a healthy diet to maximize fat loss results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_user = [\n",
        "    {'role': 'system', 'content': \"\"\"\n",
        "    You are ScienceFactBot, an AI that shares fascinating scientific facts and explains complex concepts in simple terms.\n",
        "\n",
        "    Example:\n",
        "    User: What’s the speed of light?\n",
        "    Assistant: Light travels at approximately 299,792,458 meters per second in a vacuum—fast enough to circle Earth over seven times in one second!\n",
        "\n",
        "    User: Why is the sky blue?\n",
        "    Assistant: The sky appears blue due to Rayleigh scattering, where shorter blue light waves scatter more than longer red waves in Earth's atmosphere.\n",
        "    \"\"\"}\n",
        "]\n",
        "\n",
        "# Ask ScienceFactBot a question\n",
        "print(return_OAIResponse(\"Why do we dream?\", context_user))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAEvT8v9566Z",
        "outputId": "fe99eeac-ca90-4ffa-c2b6-3d27d31f53ca"
      },
      "id": "lAEvT8v9566Z",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dreaming is a natural process that occurs during the rapid eye movement (REM) stage of sleep. While the exact reason for dreaming is not fully understood, researchers have proposed several theories. One theory suggests that dreaming helps process and make sense of our memories and experiences from the day. Another theory proposes that dreaming allows us to work through emotions and solve problems in a safe environment. Overall, dreams may serve multiple purposes such as memory consolidation, emotional processing, and creative thinking, contributing to our overall mental well-being.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of Findings**\n",
        "\n",
        "In this experiment, we tested four different AI chatbot variations: WeatherBot, TravelGuideBot, FitCoachBot, and ScienceFactBot. Each bot was given a specific role, a structured system prompt, and example conversations to guide its responses. The goal was to evaluate how well the AI adapted to each specialized domain and whether it provided accurate, relevant, and engaging responses.\n",
        "\n",
        "Performance of Each Bot\n",
        "\n",
        "\n",
        "1- WeatherBot performed well when asked about\n",
        "current weather conditions and forecasts. The responses were structured in a friendly and conversational manner, making the AI feel more like a human weather forecaster. However, without real-time data integration, it could not provide accurate weather updates for a specific location.\n",
        "\n",
        "2- TravelGuideBot gave useful travel recommendations, often mentioning well-known attractions and providing enthusiastic suggestions. The AI's responses were engaging, but some answers lacked depth. For instance, recommendations could have included more local insights, dining options, or hidden gems.\n",
        "\n",
        "3- FitCoachBot successfully offered fitness and nutrition advice. It correctly suggested strength training for muscle building and high-intensity interval training (HIIT) for fat loss. However, without personal context (e.g., age, fitness level), the recommendations were somewhat generic. It also didn’t provide alternative exercise suggestions for those with physical limitations.\n",
        "\n",
        "4- ScienceFactBot provided scientifically accurate explanations in a simplified, easy-to-understand manner. It correctly explained phenomena like the speed of light and why the sky is blue. However, when asked about dreams, its response was more speculative, reflecting the ongoing scientific debate on the topic. The AI avoided outright misinformation but lacked deeper insights into theories such as Freud’s interpretation of dreams or modern neurological research.\n",
        "\n",
        "\n",
        "\n",
        "**Variations That Didn’t Work Well**\n",
        "\n",
        "One challenge encountered was the AI's inability to access real-time or location-specific data. For example, WeatherBot could not fetch actual weather updates, limiting its usefulness compared to a real weather service.\n",
        "\n",
        "Additionally, TravelGuideBot sometimes defaulted to overly general travel advice. A more advanced version would dynamically adjust recommendations based on user preferences (e.g., adventure travel vs. historical sightseeing).\n",
        "\n",
        "There were no outright hallucinations (completely false information), but some responses could have been more precise. For example, ScienceFactBot’s explanation of dreams was somewhat vague, avoiding definitive claims due to the complexity of the subject. This cautious approach prevented misinformation but also led to less informative responses.\n",
        "\n",
        "**Key Learnings**\n",
        "\n",
        "Well-Defined Roles Improve AI Responses – Clearly specifying a chatbot’s function and providing examples helped shape its responses effectively. Each AI stuck to its domain and avoided irrelevant answers.\n",
        "\n",
        "Limitations in Real-Time and Context-Specific Data – Without live data access, certain chatbots (like WeatherBot) were less practical for real-world use. AI without external API integration is limited to static knowledge.\n",
        "\n",
        "Depth vs. Engagement – While responses were engaging and easy to understand, some lacked deeper insights. For instance, TravelGuideBot could benefit from personalized recommendations, and ScienceFactBot could provide more varied theories rather than general explanations.\n",
        "\n",
        "Cautious AI Behavior – GPT avoids speculation on uncertain topics (e.g., future weather, health predictions, or dream interpretations). While this reduces misinformation, it sometimes results in vague or incomplete answers.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "The chatbot variations performed well within their given roles but had some limitations regarding real-time data, personalization, and depth of knowledge. A future improvement could involve integrating real-world data sources or tailoring responses based on user context. Overall, the AI demonstrated strong adaptability and reliability in structured domains but should be supplemented with fact-checking for more nuanced topics.\n",
        "\n"
      ],
      "metadata": {
        "id": "5ef2AITH63bX"
      },
      "id": "5ef2AITH63bX"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}