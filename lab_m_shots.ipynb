{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "24b19fff-8f42-4e9f-a73e-00cff106805a",
      "metadata": {
        "id": "24b19fff-8f42-4e9f-a73e-00cff106805a"
      },
      "source": [
        "# M-Shots Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34723a72-1601-4685-a0ba-bff544425d48",
      "metadata": {
        "id": "34723a72-1601-4685-a0ba-bff544425d48"
      },
      "source": [
        "In this notebook, we'll explore small prompt engineering techniques and recommendations that will help us elicit responses from the models that are better suited to our needs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5nns2reZOS2",
        "outputId": "a651eb36-c207-4db1-b28d-b731bc1386b8"
      },
      "id": "R5nns2reZOS2",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9emkaUgZRDp",
        "outputId": "9fcbf592-dae0-4514-8e2f-e1e604ca0034"
      },
      "id": "a9emkaUgZRDp",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.75.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "import os\n",
        "load_dotenv('/content/.env')\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "print(\"✅ OPENAI Key loaded:\", OPENAI_API_KEY is not None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXJZao3fZZou",
        "outputId": "3b34a86b-e801-4355-984f-9223a3ed0f77"
      },
      "id": "PXJZao3fZZou",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ OPENAI Key loaded: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fba193cc-d8a0-4ad2-8177-380204426859",
      "metadata": {
        "id": "fba193cc-d8a0-4ad2-8177-380204426859"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "502cfc93-21e0-498f-9650-37bc6ddd514d",
      "metadata": {
        "id": "502cfc93-21e0-498f-9650-37bc6ddd514d"
      },
      "source": [
        "# Formatting the answer with Few Shot Samples.\n",
        "\n",
        "To obtain the model's response in a specific format, we have various options, but one of the most convenient is to use Few-Shot Samples. This involves presenting the model with pairs of user queries and example responses.\n",
        "\n",
        "Large models like GPT-3.5 respond well to the examples provided, adapting their response to the specified format.\n",
        "\n",
        "Depending on the number of examples given, this technique can be referred to as:\n",
        "* Zero-Shot.\n",
        "* One-Shot.\n",
        "* Few-Shots.\n",
        "\n",
        "With One Shot should be enough, and it is recommended to use a maximum of six shots. It's important to remember that this information is passed in each query and occupies space in the input prompt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a8344712-06d7-4c24-83d8-f36d62926e5e",
      "metadata": {
        "id": "a8344712-06d7-4c24-83d8-f36d62926e5e"
      },
      "outputs": [],
      "source": [
        "# Function to call the model.\n",
        "def return_OAIResponse(user_message, context):\n",
        "    client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=OPENAI_API_KEY,\n",
        ")\n",
        "\n",
        "    newcontext = context.copy()\n",
        "    newcontext.append({'role':'user', 'content':\"question: \" + user_message})\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=newcontext,\n",
        "            temperature=1,\n",
        "        )\n",
        "\n",
        "    return (response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f611d73d-9330-466d-b705-543667e1b561",
      "metadata": {
        "id": "f611d73d-9330-466d-b705-543667e1b561"
      },
      "source": [
        "In this zero-shots prompt we obtain a correct response, but without formatting, as the model incorporates the information he wants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
        "outputId": "bf67f5bd-9167-469a-e1f1-b06ee8d131a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sebastian Vettel won the Formula 1 World Championship in 2010. He was driving for Red Bull Racing at the time.\n"
          ]
        }
      ],
      "source": [
        "#zero-shot\n",
        "context_user = [\n",
        "    {'role':'system', 'content':'You are an expert in F1.'}\n",
        "]\n",
        "print(return_OAIResponse(\"Who won the F1 2010?\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8",
      "metadata": {
        "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8"
      },
      "source": [
        "For a model as large and good as GPT 3.5, a single shot is enough to learn the output format we expect.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
        "outputId": "510b533c-b16e-47f4-8f93-988b039694bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: Sebastian Vettel.\n",
            "Team: Red Bull Racing.\n"
          ]
        }
      ],
      "source": [
        "#one-shot\n",
        "context_user = [\n",
        "    {'role':'system', 'content':\n",
        "     \"\"\"You are an expert in F1.\n",
        "\n",
        "     Who won the 2000 f1 championship?\n",
        "     Driver: Michael Schumacher.\n",
        "     Team: Ferrari.\"\"\"}\n",
        "]\n",
        "print(return_OAIResponse(\"Who won the F1 2011?\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32c454a8-181b-482b-873a-81d6ffde4674",
      "metadata": {
        "id": "32c454a8-181b-482b-873a-81d6ffde4674"
      },
      "source": [
        "Smaller models, or more complicated formats, may require more than one shot. Here a sample with two shots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
        "outputId": "e3cd05ea-64fb-4a9d-db2c-a9113011920f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: Fernando Alonso.\n",
            "Team: Renault.\n"
          ]
        }
      ],
      "source": [
        "#Few shots\n",
        "context_user = [\n",
        "    {'role':'system', 'content':\n",
        "     \"\"\"You are an expert in F1.\n",
        "\n",
        "     Who won the 2010 f1 championship?\n",
        "     Driver: Sebastian Vettel.\n",
        "     Team: Red Bull Renault.\n",
        "\n",
        "     Who won the 2009 f1 championship?\n",
        "     Driver: Jenson Button.\n",
        "     Team: BrawnGP.\"\"\"}\n",
        "]\n",
        "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
        "outputId": "590c3b55-206e-4588-910f-297ec3f24394"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: Lewis Hamilton.\n",
            "Team: Mercedes.\n"
          ]
        }
      ],
      "source": [
        "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86",
      "metadata": {
        "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86"
      },
      "source": [
        "We've been creating the prompt without using OpenAI's roles, and as we've seen, it worked correctly.\n",
        "\n",
        "However, the proper way to do this is by using these roles to construct the prompt, making the model's learning process even more effective.\n",
        "\n",
        "By not feeding it the entire prompt as if they were system commands, we enable the model to learn from a conversation, which is more realistic for it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
        "outputId": "ac5210d7-9ce0-4b38-a675-84ce9623262a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: Lewis Hamilton. \n",
            "Team: Mercedes. \n",
            "Points: 413.\n"
          ]
        }
      ],
      "source": [
        "#Recomended solution\n",
        "context_user = [\n",
        "    {'role':'system', 'content':'You are and expert in f1.\\n\\n'},\n",
        "    {'role':'user', 'content':'Who won the 2010 f1 championship?'},\n",
        "    {'role':'assistant', 'content':\"\"\"Driver: Sebastian Vettel. \\nTeam: Red Bull. \\nPoints: 256. \"\"\"},\n",
        "    {'role':'user', 'content':'Who won the 2009 f1 championship?'},\n",
        "    {'role':'assistant', 'content':\"\"\"Driver: Jenson Button. \\nTeam: BrawnGP. \\nPoints: 95. \"\"\"},\n",
        "]\n",
        "\n",
        "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac6f6b42-f351-496b-a7e8-1286426457eb",
      "metadata": {
        "id": "ac6f6b42-f351-496b-a7e8-1286426457eb"
      },
      "source": [
        "We could also address it by using a more conventional prompt, describing what we want and how we want the format.\n",
        "\n",
        "However, it's essential to understand that in this case, the model is following instructions, whereas in the case of use shots, it is learning in real-time during inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
        "outputId": "ff845f9b-0433-4d0b-ca98-60bfda47b154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive: Lewis Hamilton\n",
            "Team: Mercedes\n",
            "Points: 413\n"
          ]
        }
      ],
      "source": [
        "context_user = [\n",
        "    {'role':'system', 'content':\"\"\"You are and expert in f1.\n",
        "    You are going to answer the question of the user giving the name of the rider,\n",
        "    the name of the team and the points of the champion, following the format:\n",
        "    Drive:\n",
        "    Team:\n",
        "    Points: \"\"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "KNDL1GzVngyL",
      "metadata": {
        "id": "KNDL1GzVngyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc2aae44-c3aa-4c0d-b729-cc3b82efbdcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: Fernando Alonso.\n",
            "Team: Renault.\n"
          ]
        }
      ],
      "source": [
        "context_user = [\n",
        "    {'role':'system', 'content':\n",
        "     \"\"\"You are classifying .\n",
        "\n",
        "     Who won the 2010 f1 championship?\n",
        "     Driver: Sebastian Vettel.\n",
        "     Team: Red Bull Renault.\n",
        "\n",
        "     Who won the 2009 f1 championship?\n",
        "     Driver: Jenson Button.\n",
        "     Team: BrawnGP.\"\"\"}\n",
        "]\n",
        "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qZPNTLMPnkQ4",
      "metadata": {
        "id": "qZPNTLMPnkQ4"
      },
      "source": [
        "Few Shots for classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ejcstgTxnnX5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejcstgTxnnX5",
        "outputId": "01950b22-10c1-43b4-f051-6473c9f0768a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: Negative\n"
          ]
        }
      ],
      "source": [
        "context_user = [\n",
        "    {'role':'system', 'content':\n",
        "     \"\"\"You are an expert in reviewing product opinions and classifying them as positive or negative.\n",
        "\n",
        "     It fulfilled its function perfectly, I think the price is fair, I would buy it again.\n",
        "     Sentiment: Positive\n",
        "\n",
        "     It didn't work bad, but I wouldn't buy it again, maybe it's a bit expensive for what it does.\n",
        "     Sentiment: Negative.\n",
        "\n",
        "     I wouldn't know what to say, my son uses it, but he doesn't love it.\n",
        "     Sentiment: Neutral\n",
        "     \"\"\"}\n",
        "]\n",
        "print(return_OAIResponse(\"I'm not going to return it, but I don't plan to buy it again.\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffe1d50b-d262-4e74-8f2d-3559f3fcfb15",
      "metadata": {
        "id": "ffe1d50b-d262-4e74-8f2d-3559f3fcfb15"
      },
      "source": [
        "# Exercise\n",
        " - Complete the prompts similar to what we did in class.\n",
        "     - Try at least 3 versions\n",
        "     - Be creative\n",
        " - Write a one page report summarizing your findings.\n",
        "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
        " - What did you learn?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "_ = load_dotenv(find_dotenv())  # Load local .env file\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Function to get completion from OpenAI\n",
        "def return_OAIResponse(user_message, context):\n",
        "    newcontext = context.copy()\n",
        "    newcontext.append({'role':'user', 'content': user_message})\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=newcontext,\n",
        "        temperature=1,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Few-shot context for classifying product reviews\n",
        "context_user = [\n",
        "    {'role': 'system', 'content': \"\"\"You are an expert in reviewing product opinions and classifying them as positive or negative.\n",
        "    It fulfilled its function perfectly, I think the price is fair, I would buy it again.\n",
        "    Sentiment: Positive\n",
        "    It didn't work bad, but I wouldn't buy it again, maybe it's a bit expensive for what it does.\n",
        "    Sentiment: Negative.\n",
        "    I wouldn't know what to say, my son uses it, but he doesn't love it.\n",
        "    Sentiment: Neutral\n",
        "    \"\"\"}\n",
        "]\n",
        "\n",
        "# First product review example\n",
        "response1 = return_OAIResponse(\"The product works, but I feel like it’s a little overpriced.\", context_user)\n",
        "print(f\"Response 1: {response1}\")\n",
        "\n",
        "# Second product review example\n",
        "response2 = return_OAIResponse(\"It's a great value for money. Highly recommend it!\", context_user)\n",
        "print(f\"Response 2: {response2}\")\n",
        "\n",
        "# Third product review example\n",
        "response3 = return_OAIResponse(\"I’m not impressed. It did the job but nothing special.\", context_user)\n",
        "print(f\"Response 3: {response3}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j6AUQi2bUJZ",
        "outputId": "e9453209-65ae-46c3-87ca-e157c4749d93"
      },
      "id": "9j6AUQi2bUJZ",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response 1: Sentiment: Negative\n",
            "Response 2: Sentiment: Positive\n",
            "Response 3: Sentiment: Neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_user = [\n",
        "    {'role': 'system', 'content': \"\"\"You are an expert in classifying product reviews as positive or negative.\n",
        "    It worked as expected and I am happy with the purchase.\n",
        "    Sentiment: Positive\n",
        "    It broke down after a week, I would not recommend it.\n",
        "    Sentiment: Negative\n",
        "    \"\"\"}\n",
        "]\n"
      ],
      "metadata": {
        "id": "S0NzRXUHbZEB"
      },
      "id": "S0NzRXUHbZEB",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_user = [\n",
        "    {'role': 'system', 'content': \"\"\"You are an expert in classifying product reviews as positive, negative, or neutral.\n",
        "    It worked well and was a great buy.\n",
        "    Sentiment: Positive\n",
        "    It didn’t meet my expectations, it’s not worth the price.\n",
        "    Sentiment: Negative\n",
        "    It’s just average, not bad, but not great either.\n",
        "    Sentiment: Neutral\n",
        "    \"\"\"}\n",
        "]\n"
      ],
      "metadata": {
        "id": "sELOjs80bbfe"
      },
      "id": "sELOjs80bbfe",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_user = [\n",
        "    {'role': 'system', 'content': \"\"\"You are an expert in classifying product reviews as positive, negative, or neutral.\n",
        "    The phone’s camera is amazing, but the battery life is a bit short.\n",
        "    Sentiment: Neutral\n",
        "    The laptop works great for all my needs and has an excellent screen.\n",
        "    Sentiment: Positive\n",
        "    The tablet freezes all the time, and the screen is unresponsive.\n",
        "    Sentiment: Negative\n",
        "    \"\"\"}\n",
        "]\n"
      ],
      "metadata": {
        "id": "J1vg3DZUbc1J"
      },
      "id": "J1vg3DZUbc1J",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Report:\n",
        "Summary:\n",
        "\n",
        "I created three versions of Few-Shot Learning with the goal of classifying product opinions. Each version used different amounts of examples and varying complexities in the instructions. Here’s a breakdown:\n",
        "\n",
        "Version 1: Simple Positive/Negative Classification\n",
        "\n",
        "Purpose: This version focused only on two sentiment categories: Positive and Negative.\n",
        "Results: The model performed well in identifying whether the review was favorable or unfavorable. It was able to classify \"works as expected\" as Positive and \"not worth the price\" as Negative.\n",
        "Version 2: Added Neutral Category\n",
        "\n",
        "Purpose: Added the Neutral category to allow for a wider range of reviews.\n",
        "Results: This version improved the model’s ability to handle reviews that were neither entirely positive nor negative, such as those that were \"just average.\"\n",
        "Version 3: More Detailed Examples\n",
        "\n",
        "Purpose: I introduced more detailed and feature-specific examples, making the sentiment analysis more nuanced.\n",
        "Results: The model handled the reviews well, classifying reviews that mentioned specific product features (e.g., camera, battery life) as neutral if they contained both positive and negative aspects.\n",
        "Findings:\n",
        "\n",
        "Strengths:\n",
        "The model adapted quickly to new instructions and could classify reviews into multiple sentiment categories, even with subtle differences.\n",
        "The addition of Neutral and detailed examples improved the model's ability to understand complex reviews.\n",
        "Issues:\n",
        "In Version 1, the model could occasionally misclassify reviews that had mixed sentiments, especially when the user was less specific.\n",
        "The model sometimes confused neutral sentiments as negative when the review mentioned minor complaints.\n",
        "What I Learned:\n",
        "\n",
        "Few-Shot Learning can significantly improve the model’s performance by providing it with clear examples of how to classify specific cases.\n",
        "The more context and examples provided, the more accurate and nuanced the model’s responses become. However, excessive complexity in examples may lead to confusion or misclassification in certain cases.\n",
        "This implementation demonstrates how powerful Few-Shot Learning can be in adapting the model to specific use cases. You can experiment with this approach by fine-tuning the examples and instructions to fit more complex needs!"
      ],
      "metadata": {
        "id": "uhKuVkMXbj0K"
      },
      "id": "uhKuVkMXbj0K"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}